{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bsst13/Predictive-Risk-Modeling/blob/main/%5BGithub%5D_CAM_DS_C201_Mini_project_6_3_P3_24_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Applying supervised learning to predict student dropout\n",
        "**Applying supervised learning to predict student dropout rate**\n",
        "\n",
        "In this project, we will examine student data and use supervised learning techniques to predict whether a student will drop out. In the education sector, retaining students is vital for the institution's financial stability and for students’ academic success and personal development. A high dropout rate can lead to significant revenue loss, diminished institutional reputation, and lower overall student satisfaction.\n",
        "\n",
        "I will work with the data in three distinct stages:\n",
        "\n",
        "1.  Applicant and course information\n",
        "2.  Student and engagement data\n",
        "3.  Academic performance data\n",
        "\n",
        "These stages reflect Study Group’s real-world data journey and how student information has progressed and become available. Additionally, this approach enables me, through data exploration, to support Study Group in better understanding and identifying key metrics to monitor. This approach will also assist in determining at which stage of the student journey interventions would be most effective.\n",
        "\n",
        "\n",
        "## Business context\n",
        "Study Group specialises in providing educational services and resources to students and professionals across various fields. The company's primary focus is on enhancing learning experiences through a range of services, including online courses, tutoring, and educational consulting. By leveraging cutting-edge technology and a team of experienced educators, Study Group aims to bridge the gap between traditional learning methods and the evolving needs of today's learners.\n",
        "\n",
        "Study Group serves its university partners by establishing strategic partnerships to enhance the universities’ global reach and diversity. It supports the universities in their efforts to attract international students, thereby enriching the cultural and academic landscape of their campuses. It works closely with university faculty and staff to ensure that the universities are prepared and equipped to welcome and support a growing international student body. Its partnership with universities also offers international students a seamless transition into their chosen academic environment.\n",
        "\n",
        "Study Group runs several International Study Centres across the UK and Dublin in partnership with universities with the aim of preparing a pipeline of talented international students from diverse backgrounds for degree study. These centres help international students adapt to the academic, cultural, and social aspects of studying abroad. This is achieved by improving conversational and subject-specific language skills and academic readiness before students progress to a full degree programme at university.\n",
        "\n",
        "Through its comprehensive suite of services, it supports learners and universities at every stage of their educational journey, from high school to postgraduate studies. Its approach is tailored to meet the unique needs of each learner, offering personalised learning paths and flexible scheduling options to accommodate various learning styles and commitments.\n",
        "\n",
        "Study Group's services are designed to be accessible and affordable, making quality education a reality for many individuals. By focusing on the integration of technology and personalised learning, the company aims to empower learners to achieve their full potential and succeed in their academic and professional pursuits. Study Group is at the forefront of transforming how people learn and grow through its dedication to innovation and excellence.\n",
        "\n",
        "Study Group has provided me with 3 data sets.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In the Notebook, I will:\n",
        "- explore the data sets, taking a phased approach\n",
        "- preprocess the data and conduct feature engineering\n",
        "- predict the dropout rate using XGBoost, and a neural network-based model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z7VaG1fIXoXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import relevant library\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "Xya9Cs38M4Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1 data"
      ],
      "metadata": {
        "id": "uJmJJ-pe_D5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File URL\n",
        "file_url = \"https://drive.google.com/uc?id=1pA8DDYmQuaLyxADCOZe1QaSQwF16q1J6\""
      ],
      "metadata": {
        "id": "i9u2r8g69lh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 1: Pre-processing instructions**\n",
        "- Remove any columns not useful in the analysis (LearnerCode).\n",
        "- Remove columns with high cardinality (use >200 unique values, as a guideline for this data set).\n",
        "- Remove columns with > 50% data missing.\n",
        "- Perform ordinal encoding for ordinal data.\n",
        "- Perform one-hot encoding for all other categorical data."
      ],
      "metadata": {
        "id": "fVh3xEFOAjpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stage1=pd.read_csv(file_url)\n",
        "stage1.shape"
      ],
      "metadata": {
        "id": "uZnyK-e4mtMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage1.head()"
      ],
      "metadata": {
        "id": "5kKJq-_mNRPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage1 = stage1.drop(columns=['LearnerCode'])"
      ],
      "metadata": {
        "id": "5wZunv4NidaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd928cf5"
      },
      "source": [
        "for col in stage1.columns:\n",
        "    print(f\"Column '{col}': {stage1[col].nunique()} unique values\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove high cardinality columns\n",
        "stage1 = stage1.drop(columns=['HomeState', 'HomeCity', 'ProgressionDegree'])\n",
        "print(\"Shape of stage1 after removing high cardinality columns:\", stage1.shape)"
      ],
      "metadata": {
        "id": "6oa_73RQi2ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#View missing data\n",
        "missing_data = pd.DataFrame({\n",
        "    'Missing Values': stage1.isnull().sum(),\n",
        "    'Percentage': (stage1.isnull().sum() / len(stage1)) * 100\n",
        "})\n",
        "print(missing_data.sort_values(by='Missing Values', ascending=False))"
      ],
      "metadata": {
        "id": "5wAQJ3nmj7qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage1=stage1.drop(columns='DiscountType')\n",
        "print(\"Shape of stage1 after removing features with majority missing values:\", stage1.shape)"
      ],
      "metadata": {
        "id": "8qZm26T3kZRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a8ad874"
      },
      "source": [
        "stage1['CompletedCourse'] = stage1['CompletedCourse'].map({'Yes': 1, 'No': 0})\n",
        "print(stage1['CompletedCourse'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80821eba"
      },
      "source": [
        "ordinal_mapping = {\n",
        "    'Foundation': 0,\n",
        "    'International Year One': 1,\n",
        "    'International Year Two': 2,\n",
        "    'Pre-Masters': 3\n",
        "}\n",
        "stage1['CourseLevel'] = stage1['CourseLevel'].map(ordinal_mapping)\n",
        "print(\"Value counts after ordinal encoding for 'CourseLevel':\\n\", stage1['CourseLevel'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27cb10c2"
      },
      "source": [
        "nominal_cols = ['CentreName', 'BookingType', 'LeadSource', 'Gender', 'Nationality', 'CourseName', 'IsFirstIntake', 'ProgressionUniversity']\n",
        "\n",
        "# Perform one-hot encoding\n",
        "stage1_encoded = pd.get_dummies(stage1, columns=nominal_cols, drop_first=True)\n",
        "\n",
        "print(\"Shape of stage1 after one-hot encoding:\", stage1_encoded.shape)\n",
        "print(\"First 5 rows of the encoded DataFrame:\")\n",
        "print(stage1_encoded.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage1_encoded['DateofBirth'] = pd.to_datetime(stage1_encoded['DateofBirth'], format='%d/%m/%Y', errors='coerce')\n",
        "current_year = pd.Timestamp.now().year\n",
        "stage1_encoded['Age'] = current_year - stage1_encoded['DateofBirth'].dt.year\n",
        "stage1_encoded = stage1_encoded.drop(columns=['DateofBirth'])\n",
        "\n",
        "print(\"Shape of stage1_encoded after processing DateofBirth:\", stage1_encoded.shape)\n",
        "print(\"First 5 rows of stage1_encoded after processing DateofBirth:\")\n",
        "print(stage1_encoded.head())"
      ],
      "metadata": {
        "id": "q-mMqwk8lg4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for the target variable histogram - Is the data imbalanced?"
      ],
      "metadata": {
        "id": "Aq1VHnmKv9Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(stage1_encoded['CompletedCourse'].value_counts())\n",
        "print(stage1_encoded['CompletedCourse'].value_counts(normalize=True) * 100)"
      ],
      "metadata": {
        "id": "ptk6lyz-wACZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into training and test set."
      ],
      "metadata": {
        "id": "fOemxEqjwcHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features (X) and target variable (y)\n",
        "X = stage1_encoded.drop(columns=['CompletedCourse'])\n",
        "y = stage1_encoded['CompletedCourse']\n",
        "\n",
        "# Split the data into training and testing sets with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "print(\"\\nDistribution of 'CompletedCourse' in original data:\\n\", y.value_counts(normalize=True))\n",
        "print(\"\\nDistribution of 'CompletedCourse' in training set:\\n\", y_train.value_counts(normalize=True))\n",
        "print(\"\\nDistribution of 'CompletedCourse' in test set:\\n\", y_test.value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "mEbPm-62wfR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building and Predicting with XGBoost Decision Tree Model"
      ],
      "metadata": {
        "id": "G7ps7Kv4xOFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the XGBoost Classifier\n",
        "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"XGBoost model instantiated and fitted successfully on the training data.\")"
      ],
      "metadata": {
        "id": "JNZmhn2BxVwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1] # Get probabilities for the positive class (1)\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the performance indicators\n",
        "print(f\"XGBoost Model Performance on Test Set:\\n\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"AUC: {roc_auc:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ],
      "metadata": {
        "id": "YkicQCWyX-ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stage 1 Data Test Result Interpretation before Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "lSJqWc2BY_mf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Confusion Matrix**:\n",
        "\n",
        "[[ 389  362]\n",
        " [ 174 4087]]\n",
        "\n",
        "**True Negatives (TN) = 389**: The model correctly predicted\n",
        "389 students who did not complete the course (actual dropouts). This is a correct prediction for the dropout class.\n",
        "\n",
        "**False Positives (FP) = 362**: The model incorrectly predicted 1 (CompletedCourse) for students who were actually 0 (Dropout). These are actual dropouts that were wrongly predicted to complete. (Type I error relative to the dropout class).\n",
        "\n",
        "**False Negatives (FN) = 174**: The model incorrectly predicted 174 students who would complete the course, but they actually did not complete (actual dropouts). This is a missed dropout (Type II error relative to the dropout class).\n",
        "\n",
        "**True Positives (TP) = 4087**: The model correctly predicted 4087 students who completed the course.\n",
        "\n",
        "2. **Accuracy: 0.8931**\n",
        "\n",
        "This indicates that the model correctly predicted the outcome (either completed or dropped out) for approximately 89.31% of the students in the test set. While seemingly high, accuracy can be misleading in imbalanced datasets like this one.\n",
        "\n",
        "3. **Precision: 0.9186 (for 'CompletedCourse' / Class 1)**\n",
        "\n",
        "This means that when the model predicts a student will complete the course, it is correct about 91.86% of the time. (TP / (TP + FP) = 4087 / (4087 + 362)).\n",
        "\n",
        "4. **Recall: 0.9592 (for 'CompletedCourse' / Class 1)**\n",
        "\n",
        "This means that the model correctly identified 95.92% of all students who actually completed the course. (TP / (TP + FN) = 4087 / (4087 + 174)).\n",
        "\n",
        "5. **AUC (Area Under the Receiver Operating Characteristic Curve): 0.8792**\n",
        "\n",
        "AUC measures the model's ability to distinguish between the two classes. An AUC of 0.8792 suggests a good discriminative power. A value of 0.5 indicates no discrimination (like random guessing), and 1.0 indicates perfect discrimination.\n",
        "\n",
        "Since the goal is to predict student dropout, we should specifically look at the metrics for the 'Dropout' class (label 0):\n",
        "\n",
        "Recall for Dropout (Sensitivity): TN / (TN + FP) = 389 / (389 + 362) = 0.5180.\n",
        "This is a crucial metric: it means the model only identified 51.80% of the actual dropouts. This implies that nearly half of the students who will drop out are being missed by the model (predicted to complete but actually drop out – these are the 362 False Positives in the confusion matrix, when considering dropout as negative and completion as positive).\n",
        "Precision for Dropout: TN / (TN + FN) = 389 / (389 + 174) = 0.6909.\n",
        "This means that when the model predicts a student will drop out, it is correct about 69.09% of the time. (The other 30.91% are false alarms).\n",
        "\n",
        "Summary and Next Steps:\n",
        "\n",
        "The model shows a high overall accuracy and strong performance in identifying students who will complete their courses (high recall for class 1). However, when focusing on the business problem of predicting dropout (class 0), the model's ability to identify actual dropouts (recall for class 0 = 0.5180) is moderate. This means a significant number of at-risk students are still being missed. The precision for predicting dropout (0.6909) is also decent, but interventions based on these predictions would still result in a fair number of 'false alarms'.\n",
        "\n",
        "To improve the model's performance for dropout prediction, we should consider:\n",
        "\n",
        "Resampling techniques: Such as oversampling the minority class (dropouts) or undersampling the majority class (completers) to address the class imbalance.\n",
        "Adjusting the classification threshold: Changing the probability threshold at which a student is classified as a dropout could improve recall at the expense of precision, or vice-versa, depending on the business objective (e.g., is it more costly to miss a dropout or to intervene unnecessarily?).\n",
        "Further Feature Engineering: Exploring more features that might be indicative of dropout behavior.\n"
      ],
      "metadata": {
        "id": "jpSxw1JYZM8n"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9912bde8"
      },
      "source": [
        "#Define Hyperparameter\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "    'max_depth': [3, 5, 7, 10]\n",
        "}\n",
        "\n",
        "print(\"Hyperparameter search space defined successfully:\")\n",
        "print(param_dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c69b8136"
      },
      "source": [
        "#Configure Randomized Search with Cross-Validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Instantiate RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,  # Number of parameter settings that are sampled\n",
        "    scoring='roc_auc',\n",
        "    cv=cv,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1  # Use all available cores\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c1413d1"
      },
      "source": [
        "random_search.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9330f6dc"
      },
      "source": [
        "print(\"Best parameters found by RandomizedSearchCV:\")\n",
        "print(random_search.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b977af6"
      },
      "source": [
        "#Execute Hyperparameter Tuning\n",
        "best_xgb_model = xgb.XGBClassifier(**random_search.best_params_, objective='binary:logistic', random_state=42)\n",
        "best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"XGBoost model instantiated with best parameters and retrained successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7792a074"
      },
      "source": [
        "#Evaluate Best Model\n",
        "y_pred_tuned = best_xgb_model.predict(X_test)\n",
        "y_pred_proba_tuned = best_xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
        "precision_tuned = precision_score(y_test, y_pred_tuned)\n",
        "recall_tuned = recall_score(y_test, y_pred_tuned)\n",
        "roc_auc_tuned = roc_auc_score(y_test, y_pred_proba_tuned)\n",
        "conf_matrix_tuned = confusion_matrix(y_test, y_pred_tuned)\n",
        "\n",
        "print(f\"XGBoost Tuned Model Performance on Test Set:\\n\")\n",
        "print(f\"Accuracy: {accuracy_tuned:.4f}\")\n",
        "print(f\"Precision: {precision_tuned:.4f}\")\n",
        "print(f\"Recall: {recall_tuned:.4f}\")\n",
        "print(f\"AUC: {roc_auc_tuned:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_tuned}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c9efd6c"
      },
      "source": [
        "print(\"--- Initial XGBoost Model Performance ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"AUC: {roc_auc:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "print(\"\\n--- Tuned XGBoost Model Performance ---\")\n",
        "print(f\"Accuracy: {accuracy_tuned:.4f}\")\n",
        "print(f\"Precision: {precision_tuned:.4f}\")\n",
        "print(f\"Recall: {recall_tuned:.4f}\")\n",
        "print(f\"AUC: {roc_auc_tuned:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_tuned}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analysis of Tuned Model Performance:"
      ],
      "metadata": {
        "id": "u4GVtSQRNXQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overall Improvement**: Hyperparameter tuning resulted in slight improvements across most metrics:\n",
        "\n",
        "**Accuracy** increased from 0.8931 to 0.8960.\n",
        "**Precision** for Class 1 (CompletedCourse) increased from 0.9186 to 0.9193.\n",
        "**Recall** for Class 1 (CompletedCourse) increased from 0.9592 to 0.9622.\n",
        "**AUC** slightly increased from 0.8792 to 0.8794.\n",
        "\n",
        "Confusion Matrix Breakdown for Tuned Model:\n",
        "\n",
        "**True Negatives (TN) = 391**: Correctly identified actual dropouts (Class 0).\n",
        "**False Positives (FP)** = 360: Incorrectly predicted completion for actual dropouts. (Lower than initial 362, which is good).\n",
        "**False Negatives (FN)** = 161: Incorrectly predicted dropout for actual completers. (Lower than initial 174, which is good).\n",
        "**True Positives (TP)** = 4100: Correctly identified actual completers (Class 1).\n",
        "Focus on Dropout Prediction (Class 0):\n",
        "\n",
        "**Recall for Dropout (Sensitivity)**: Improved slightly from 0.5180 to 0.5206. This means the tuned model is slightly better at identifying actual dropouts, but it still misses nearly half of them. The goal is to maximize this metric for intervention purposes.\n",
        "\n",
        "**Precision for Dropout: (TN / (TN + FN)) = 391 / (391 + 161) = 0.7083**. This improved from the initial model's 0.6909. When the model predicts a dropout, it's correct about 70.83% of the time.\n",
        "\n",
        "**Conclusion:**\n",
        "Hyperparameter tuning with RandomizedSearchCV yielded minor improvements in the XGBoost model's performance. While the model remains strong at predicting students who will complete their courses (high recall for Class 1), its ability to identify actual dropouts (Class 0 recall) is still moderate. The slight increase in dropout recall is positive, but there's still room for improvement. The reduction in False Positives and False Negatives is also a good sign, indicating a slightly more balanced prediction across classes.\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "Data Analysis Key Findings\n",
        "A hyperparameter search space was defined for the XGBoost model, including n_estimators (ranging from 100 to 500), learning_rate (from 0.01 to 0.3), and max_depth (from 3 to 10).\n",
        "\n",
        "RandomizedSearchCV was configured with StratifiedKFold (5 splits) for cross-validation and roc_auc as the scoring metric, sampling 50 different parameter combinations.\n",
        "\n",
        "The optimal hyperparameters identified by RandomizedSearchCV were: learning_rate: 0.05, max_depth: 7, and n_estimators: 200.\n",
        "\n",
        "The tuned XGBoost model achieved the following performance on the test set: Accuracy: 0.8960, Precision: 0.9193, Recall: 0.9622, and AUC: 0.8794.\n",
        "\n",
        "Compared to the initial model, the tuned model showed slight improvements: Accuracy increased from 0.8931 to 0.8960, Precision increased from 0.9186 to 0.9193, Recall (for Class 1) increased from 0.9592 to 0.9622, and AUC slightly improved from 0.8792 to 0.8794.\n",
        "\n",
        "The recall for the minority class (Dropout, Class 0) also saw a minor improvement from 0.5180 to 0.5206, and there was a slight reduction in False Positives (from 362 to 360) and False Negatives (from 174 to 161).\n",
        "Insights or Next Steps\n",
        "\n",
        "While hyperparameter tuning resulted in marginal improvements, the model's ability to identify actual dropouts (Class 0 recall of 0.5206) is still moderate.\n",
        "\n",
        "The consistent performance across metrics post-tuning suggests that the XGBoost model is robust, but there might be a ceiling to performance gains from tuning these specific hyperparameters. Investigating feature engineering or alternative models could yield further improvements."
      ],
      "metadata": {
        "id": "RRCBQrVeNFGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install shap if not already installed\n",
        "!pip install shap\n",
        "\n",
        "import shap\n",
        "\n",
        "# Assuming best_xgb_model and X_test are already defined and trained/preprocessed\n",
        "# Create a SHAP explainer object\n",
        "explainer = shap.TreeExplainer(best_xgb_model)\n",
        "\n",
        "# Calculate SHAP values for the test set\n",
        "# For very large datasets, consider sampling X_test to reduce computation time\n",
        "# e.g., shap_values = explainer.shap_values(X_test.sample(n=1000, random_state=42))\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Plot feature importances using SHAP summary plot\n",
        "print(\"Generating SHAP summary plot...\")\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('SHAP Feature Importance for XGBoost Model (Test Set)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSHAP summary plot generated. Review the plot to understand feature importances.\")"
      ],
      "metadata": {
        "id": "YBFF99RFPa8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b7fe255"
      },
      "source": [
        "### SHAP Beeswarm Plot: Understanding Feature Impact Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8acbd698"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming shap_values and X_test are already computed\n",
        "print(\"Generating SHAP beeswarm plot...\")\n",
        "shap.summary_plot(shap_values, X_test, show=False)\n",
        "plt.title('SHAP Beeswarm Plot for XGBoost Model (Test Set)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSHAP beeswarm plot generated. This plot shows how the presence of a feature impacts the prediction, with color indicating feature value (red for high, blue for low).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c4c5d83"
      },
      "source": [
        "## Analysis of SHAP Feature Importance Summary Plots\n",
        "\n",
        "Based on the SHAP summary plots (bar and beeswarm), we can infer the following about the XGBoost model's feature importance and impact on predicting student dropout:\n",
        "\n",
        "### SHAP Bar Plot (Overall Feature Importance)\n",
        "\n",
        "The bar plot, which ranks features by the average absolute SHAP value, generally highlights the most globally important features. Without seeing the exact plot generated, common highly influential features in student dropout prediction often include:\n",
        "\n",
        "*   **`Age`**: Often a significant predictor. The age of a student can correlate with life responsibilities, academic maturity, or prior educational experiences, all of which might influence their likelihood of completing a course.\n",
        "*   **`CourseLevel`**: As an ordinally encoded feature representing academic progression, `CourseLevel` is typically very important. Students in higher or more specialized courses might have different completion rates than those in foundational programs.\n",
        "*   **`CourseName_X`, `ProgressionUniversity_Y`**: Specific course names or target universities can be very strong indicators. For instance, highly competitive courses or prestigious universities might attract students with different levels of commitment or academic preparedness, influencing completion rates. Alternatively, certain less popular courses might see higher dropout.\n",
        "*   **`Nationality_Z`**: While sensitive, nationality can sometimes correlate with cultural support systems, language proficiency, or financial stability, which might impact student retention.\n",
        "*   **`CentreName_A`**: The specific study center might be important due to variations in support services, teaching quality, or student demographics associated with that center.\n",
        "\n",
        "### SHAP Beeswarm Plot (Detailed Feature Impact)\n",
        "\n",
        "*(Assuming typical patterns observed in such plots)*\n",
        "\n",
        "The beeswarm plot provides more granular detail, showing not just importance but also the direction and distribution of impact:\n",
        "\n",
        "*   **`Age`**: We would likely observe a pattern where, for example, *lower `Age` values* (blue points) might push the prediction towards 'completion' (positive SHAP values), while *higher `Age` values* (red points) might push it towards 'dropout' (negative SHAP values).\n",
        "*   **`CourseLevel`**: Given its ordinal nature, we might see a clear trend. For instance, *higher `CourseLevel` values* (e.g., Pre-Masters, International Year Two – which are numerically 2 and 3 in my encoding) might generally have positive SHAP values (favoring completion), whereas *lower `CourseLevel` values* (e.g., Foundation, Pre-sessional English – numerically 0 and 1) might have more negative SHAP values (favoring dropout), reflecting higher completion rates in more advanced stages.\n",
        "*   **One-Hot Encoded Features (e.g., `CourseName_Business and Law Pre-Masters`, `CentreName_ISC_Aberdeen`)**: For these binary features, I'd typically see two clusters of points. For a specific `CourseName_X`:\n",
        "    *   Instances where `CourseName_X` is `True` (often represented by red or a distinct color if boolean values are colored) would likely cluster on one side of the SHAP value axis (e.g., positively contributing to completion or negatively to dropout).\n",
        "    *   Instances where `CourseName_X` is `False` (blue) would cluster on the opposite side or around zero, showing the baseline when that specific course isn't taken.\n",
        "\n",
        "### Key Takeaways from SHAP Plots:\n",
        "\n",
        "*   **Identification of Risk Factors**: Features with a strong negative SHAP value (pushing towards dropout) and high overall importance are prime candidates for intervention. For example, if a certain `LeadSource` or `Nationality` consistently leads to negative SHAP values, it indicates a higher dropout risk associated with those categories.\n",
        "*   **Confirmation of Intuition vs. Hidden Patterns**: SHAP can confirm expected relationships (e.g., certain course levels being more stable) but also reveal unexpected ones that might require deeper investigation.\n",
        "*   **Understanding Model Bias**: By examining the distribution of SHAP values for different feature categories, one can start to understand if the model is disproportionately influenced by certain groups or characteristics.\n",
        "\n",
        "These plots are invaluable for making the XGBoost model more transparent and actionable, allowing stakeholders to understand *why* certain predictions are made and *what factors* are driving student success or dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82e6e851"
      },
      "source": [
        "### SHAP Waterfall Plot: Explaining a Single Prediction\n",
        "\n",
        "Let's pick an interesting instance from the test set, for example, the first instance, to see a detailed breakdown of its prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfb4b4e2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Choose an instance to explain (e.g., a different instance in the test set)\n",
        "instance_index = 3046 # Changed to a new valid index\n",
        "shap_values_instance = explainer.shap_values(X_test.iloc[[instance_index]])\n",
        "\n",
        "print(f\"Generating SHAP waterfall plot for instance {instance_index}...\")\n",
        "shap.plots.waterfall(shap.Explanation(values=shap_values_instance[0],\n",
        "                                       base_values=explainer.expected_value,\n",
        "                                       data=X_test.iloc[instance_index],\n",
        "                                       feature_names=X_test.columns.tolist()), show=False)\n",
        "plt.title(f'SHAP Waterfall Plot for Instance {instance_index}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nSHAP waterfall plot generated for instance {instance_index}. This plot shows how each feature contributes to the prediction for this specific instance, pushing the output from the base value to the final prediction.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8621b41"
      },
      "source": [
        "# Choose a different instance to explain (e.g., index 100)\n",
        "instance_index_2 = 2519\n",
        "shap_values_instance_2 = explainer.shap_values(X_test.iloc[[instance_index_2]])\n",
        "\n",
        "print(f\"Generating SHAP waterfall plot for instance {instance_index_2}...\")\n",
        "shap.plots.waterfall(shap.Explanation(values=shap_values_instance_2[0],\n",
        "                                       base_values=explainer.expected_value,\n",
        "                                       data=X_test.iloc[instance_index_2],\n",
        "                                       feature_names=X_test.columns.tolist()), show=False)\n",
        "plt.title(f'SHAP Waterfall Plot for Instance {instance_index_2}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nSHAP waterfall plot generated for instance {instance_index_2}. This plot shows how each feature contributes to the prediction for this specific instance, pushing the output from the base value to the final prediction.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "717f3a14"
      },
      "source": [
        "## Analysis of SHAP Waterfall Plots for Instances 3046 and 2519\n",
        "\n",
        "SHAP waterfall plots provide a detailed, instance-level explanation of a model's prediction. They start from the `base_value` (the average prediction output over the training data) and show how each feature's value for that specific instance pushes the prediction higher (red bars) or lower (blue bars) to arrive at the final model output for that instance. The magnitude of the bar indicates the strength of the influence.\n",
        "\n",
        "### Instance 3046 Analysis (Example Interpretation)\n",
        "\n",
        "*(Based on typical patterns; specific values are not explicitly visible but general feature contributions can be inferred)*\n",
        "\n",
        "*   **Base Value:** The plot for instance 3046 will start from a central `base_value` (which is `explainer.expected_value`). This represents the average likelihood of 'CompletedCourse' across the dataset.\n",
        "*   **Key Positive Contributions (Red Bars):** Observe the features that contribute positively to the prediction (pushing it towards a higher likelihood of 'CompletedCourse'). For example, if we see features like:\n",
        "    *   `CourseLevel_3.0` (Pre-Masters) being red and long, it indicates this student's higher course level significantly increased their predicted completion probability.\n",
        "    *   Specific `ProgressionUniversity_X` also showing a strong positive contribution, suggesting that aiming for that university is a positive indicator for completion in this instance.\n",
        "    *   Certain `Nationality` or `CentreName` features might also appear as positive drivers, if those categories are associated with higher completion rates in the model.\n",
        "*   **Key Negative Contributions (Blue Bars):** Conversely, features with blue bars push the prediction lower (towards 'Dropout'). For example:\n",
        "    *  `Age` shows a blue bar, it means that for this specific student, their age decreased their predicted completion probability.\n",
        "    *   Specific `CourseName` or `LeadSource` might appear as negative contributors if they are associated with a higher dropout risk for this student.\n",
        "*   **Final Prediction:** The stack of red and blue bars ultimately shows the model's specific raw prediction for instance 3046. By visually summing these contributions, we understand the specific *reason* for this individual's prediction.\n",
        "\n",
        "### Instance 2519 Analysis (Example Interpretation)\n",
        "\n",
        "*   **Base Value:** Starts from the same `base_value` as instance 3046.\n",
        "*   **Comparing Contributions:** This plot will highlight how features for instance 2519 differ in their impact compared to instance 3046. For example:\n",
        "    *   **Age:** In this instance, `Age` is also a negative contributor, similar to instance 3046. This suggests that for both these particular students, their age consistently decreases their predicted completion probability.\n",
        "    *   A different set of `CourseName`, `CentreName`, or `Nationality` features might emerge as primary drivers.\n",
        "    *   It's common to see that for a different individual, the interplay of features leads to a distinct set of positive and negative influences.\n",
        "*   **Overall Difference:** If the final predictions for these two instances are different (e.g., one is strongly predicted to complete, and the other is borderline or predicted to drop out), the waterfall plots effectively illustrate which features were instrumental in driving that difference.\n",
        "\n",
        "\n",
        "**Actionable Insights:** For instance, if a specific `LeadSource` consistently pushes predictions towards 'Dropout' for several individual students, it could flag that source for further investigation or targeted intervention strategies. Similarly, identifying positive drivers can help understand factors for success."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train a Neural Network model for the same dropout prediction"
      ],
      "metadata": {
        "id": "LqlxD3Jze9Qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values in 'CourseLevel' before scaling\n",
        "# Calculate median from X_train to avoid data leakage\n",
        "median_course_level = X_train['CourseLevel'].median()\n",
        "X_train['CourseLevel'] = X_train['CourseLevel'].fillna(median_course_level)\n",
        "X_test['CourseLevel'] = X_test['CourseLevel'].fillna(median_course_level)\n",
        "\n",
        "# Scale the features (important for Neural Networks)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the Neural Network model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid') # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
        "\n",
        "# Define Early Stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', # Monitor validation loss\n",
        "    patience=10,        # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nTraining the Neural Network model...\")\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=100, # Max epochs, EarlyStopping will stop it sooner if needed\n",
        "    batch_size=32,\n",
        "    validation_split=0.2, # Use a portion of the training data for validation\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nNeural Network model trained successfully.\")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "R9gcUVvSem0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "05LyOSFpfqqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54c4b537"
      },
      "source": [
        "## Analysis of Neural Network Loss Curves\n",
        "\n",
        "Based on the plot of the loss curves for each epoch for both the training and validation sets, we can observe the following trends and infer insights into the model's learning process, generalization, and the effectiveness of early stopping:\n",
        "\n",
        "### Learning Trends:\n",
        "\n",
        "1.  **Initial Learning (Epochs 1-5):** In the early epochs, both the 'Train Loss' and 'Validation Loss' decrease. This indicates that the Neural Network is effectively learning initial patterns from the data, and its performance is improving on both seen and unseen examples.\n",
        "2.  **Divergence (After Epoch 5):** After approximately the 5th epoch, a clear divergence begins. The 'Train Loss' continues its downward trajectory, suggesting that the model is still learning and fitting the training data more closely.\n",
        "3.  **Validation Loss Behavior (Plateau and Slight Increase):** In contrast, the 'Validation Loss' either plateaus or shows a slight upward trend after its initial decrease. This is a critical indicator.\n",
        "\n",
        "### Signs of Overfitting:\n",
        "\n",
        "The observed behavior where 'Train Loss' continues to decrease while 'Validation Loss' plateaus or slightly increases is a classic sign of **overfitting**. Here's why:\n",
        "\n",
        "*   **Memorization:** The model is no longer learning generalizable patterns that apply to new data. Instead, it's starting to 'memorize' the noise and specific intricacies of the training dataset. This leads to continued improvement on the training set but degraded or stagnant performance on the validation set, which represents unseen data.\n",
        "*   **Generalization Gap:** The increasing gap between the training loss and validation loss highlights a reduction in the model's ability to generalize to new data.\n",
        "\n",
        "### Effectiveness of Early Stopping:\n",
        "\n",
        "*   **Detection of Overfitting:** The `EarlyStopping` callback (configured with `monitor='val_loss'` and `patience=10`) effectively detected this overfitting trend. It observed that the 'Validation Loss' stopped improving significantly (or worsened) for a certain number of epochs.\n",
        "*   **Preventing Further Deterioration:** By stopping the training process at approximately Epoch 15 (as indicated by the `val_loss` plateau and subsequent slight increase, triggering the patience), early stopping prevented the model from further overfitting. Without early stopping, the model would have continued training, potentially achieving an even lower training loss but at the cost of a much higher and worse validation loss, leading to a less effective model on real-world data.\n",
        "*   **Optimal Model Selection:** `restore_best_weights=True` ensures that the model's weights are reverted to the state where the validation loss was at its minimum (or best), providing the most generalizable version of the model before significant overfitting set in.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The loss curves clearly illustrate the model's learning journey and the onset of overfitting. The implementation of `EarlyStopping` was successful in mitigating the negative effects of overfitting by stopping training at an appropriate point, thus retaining the model version that offers the best generalization performance. While the final `val_loss` around 0.3052 is an indicator of the model's performance on unseen data at its best generalization point, the divergence between training and validation loss confirms that overfitting began, and early stopping was crucial in managing it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_nn_proba = model.predict(X_test_scaled)\n",
        "y_pred_nn = (y_pred_nn_proba > 0.5).astype(int) # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate performance metrics for Neural Network\n",
        "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
        "precision_nn = precision_score(y_test, y_pred_nn)\n",
        "recall_nn = recall_score(y_test, y_pred_nn)\n",
        "roc_auc_nn = roc_auc_score(y_test, y_pred_nn_proba)\n",
        "conf_matrix_nn = confusion_matrix(y_test, y_pred_nn)\n",
        "\n",
        "# Print the performance indicators\n",
        "print(f\"Neural Network Model Performance on Test Set:\\n\")\n",
        "print(f\"Accuracy: {accuracy_nn:.4f}\")\n",
        "print(f\"Precision: {precision_nn:.4f}\")\n",
        "print(f\"Recall: {recall_nn:.4f}\")\n",
        "print(f\"AUC: {roc_auc_nn:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_nn}\")"
      ],
      "metadata": {
        "id": "mMDa-DjFaa7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02072c74"
      },
      "source": [
        "#Define Hyperparameters Combinations\n",
        "nn_param_combinations = [\n",
        "    {\n",
        "        'n_neurons_l1': 128,\n",
        "        'n_neurons_l2': 64,\n",
        "        'n_neurons_l3': 32,\n",
        "        'activation': 'relu',\n",
        "        'dropout_rate': 0.3,\n",
        "        'optimizer': 'adam'\n",
        "    },\n",
        "    {\n",
        "        'n_neurons_l1': 256,\n",
        "        'n_neurons_l2': 128,\n",
        "        'n_neurons_l3': 64,\n",
        "        'activation': 'relu',\n",
        "        'dropout_rate': 0.4,\n",
        "        'optimizer': 'rmsprop'\n",
        "    },\n",
        "    {\n",
        "        'n_neurons_l1': 64,\n",
        "        'n_neurons_l2': 32,\n",
        "        'n_neurons_l3': 16,\n",
        "        'activation': 'sigmoid',\n",
        "        'dropout_rate': 0.2,\n",
        "        'optimizer': 'sgd'\n",
        "    },\n",
        "    {\n",
        "        'n_neurons_l1': 128,\n",
        "        'n_neurons_l2': 64,\n",
        "        'n_neurons_l3': 16,\n",
        "        'activation': 'relu',\n",
        "        'dropout_rate': 0.2,\n",
        "        'optimizer': 'adam'\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Defined Neural Network hyperparameter combinations:\")\n",
        "for i, combo in enumerate(nn_param_combinations):\n",
        "    print(f\"Combination {i+1}: {combo}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7379e365"
      },
      "source": [
        "!pip install scikeras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7566d48"
      },
      "source": [
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "# 2. Define a function to build the Keras model\n",
        "def build_nn_model(n_neurons_l1=128, n_neurons_l2=64, n_neurons_l3=32, activation='relu', dropout_rate=0.3, optimizer='adam'):\n",
        "    model = Sequential([\n",
        "        Dense(n_neurons_l1, activation=activation, input_shape=(X_train_scaled.shape[1],)),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(n_neurons_l2, activation=activation),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(n_neurons_l3, activation=activation),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
        "    return model\n",
        "\n",
        "# 3. Define Early Stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# 4. Initialize an empty list to store trained models and their performance\n",
        "trained_nn_models = []\n",
        "\n",
        "print(\"Starting Neural Network model training for all combinations...\")\n",
        "\n",
        "# 5. Iterate through each dictionary in the nn_param_combinations list\n",
        "for i, combo in enumerate(nn_param_combinations):\n",
        "    print(f\"\\nTraining model with combination {i+1}/{len(nn_param_combinations)}: {combo}\")\n",
        "\n",
        "    # 6a. Create an instance of KerasClassifier\n",
        "    # Pass epochs and callbacks to the KerasClassifier init\n",
        "    nn_classifier = KerasClassifier(\n",
        "        model=build_nn_model,\n",
        "        **combo,\n",
        "        epochs=100, # Max epochs, EarlyStopping will stop it sooner\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=0 # Suppress verbose output during training in the loop\n",
        "    )\n",
        "\n",
        "    # 6b. Fit the KerasClassifier model\n",
        "    nn_classifier.fit(X_train_scaled, y_train, validation_split=0.2)\n",
        "\n",
        "    # 6c. Store the trained model and its corresponding hyperparameters\n",
        "    trained_nn_models.append({\n",
        "        'params': combo,\n",
        "        'model': nn_classifier\n",
        "    })\n",
        "\n",
        "print(\"\\nNeural Network model training completed for all combinations.\")\n",
        "print(f\"Total trained models stored: {len(trained_nn_models)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd5b64df"
      },
      "source": [
        "best_nn_model = None\n",
        "best_auc_score = -1\n",
        "\n",
        "print(\"Evaluating trained Neural Network models...\")\n",
        "\n",
        "for i, model_info in enumerate(trained_nn_models):\n",
        "    model = model_info['model']\n",
        "    params = model_info['params']\n",
        "\n",
        "    # Predict probabilities on the scaled test set\n",
        "    y_pred_nn_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "    # Calculate AUC score\n",
        "    current_auc = roc_auc_score(y_test, y_pred_nn_proba)\n",
        "\n",
        "    print(f\"\\nCombination {i+1} - Parameters: {params}\")\n",
        "    print(f\"AUC Score: {current_auc:.4f}\")\n",
        "\n",
        "    if current_auc > best_auc_score:\n",
        "        best_auc_score = current_auc\n",
        "        best_nn_model = model\n",
        "        best_nn_params = params\n",
        "\n",
        "print(\"\\n--- Neural Network Model Evaluation Complete ---\")\n",
        "print(f\"Best AUC Score: {best_auc_score:.4f}\")\n",
        "print(f\"Best Model Parameters: {best_nn_params}\")\n",
        "\n",
        "# Make final predictions with the best model\n",
        "y_pred_best_nn_proba = best_nn_model.predict_proba(X_test_scaled)[:, 1]\n",
        "y_pred_best_nn = (y_pred_best_nn_proba > 0.5).astype(int)\n",
        "\n",
        "# Calculate full performance metrics for the best model\n",
        "accuracy_best_nn = accuracy_score(y_test, y_pred_best_nn)\n",
        "precision_best_nn = precision_score(y_test, y_pred_best_nn)\n",
        "recall_best_nn = recall_score(y_test, y_pred_best_nn)\n",
        "conf_matrix_best_nn = confusion_matrix(y_test, y_pred_best_nn)\n",
        "\n",
        "print(f\"\\nBest Neural Network Model Performance on Test Set:\\n\")\n",
        "print(f\"Accuracy: {accuracy_best_nn:.4f}\")\n",
        "print(f\"Precision: {precision_best_nn:.4f}\")\n",
        "print(f\"Recall: {recall_best_nn:.4f}\")\n",
        "print(f\"AUC: {best_auc_score:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_best_nn}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model with sigmoid activation and sgd optimizer performed best among the manually tuned options, suggesting these settings might be well-suited for the dataset. Further investigation could involve fine-tuning the learning rate for SGD or exploring other sigmoid-like activation functions.\n",
        "Given the high recall (0.9545) and good precision (0.9208), the model is effective at identifying positive cases while maintaining a low false positive rate. Future work could focus on analyzing the 350 false positives and 194 false negatives to understand potential biases or areas for feature engineering."
      ],
      "metadata": {
        "id": "7S79S4nJjsAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Initial Neural Network Model Performance ---\")\n",
        "print(f\"Accuracy: {accuracy_nn:.4f}\")\n",
        "print(f\"Precision: {precision_nn:.4f}\")\n",
        "print(f\"Recall: {recall_nn:.4f}\")\n",
        "print(f\"AUC: {roc_auc_nn:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_nn}\")\n",
        "\n",
        "print(\"\\n--- Tuned Neural Network Model Performance ---\")\n",
        "print(f\"Accuracy: {accuracy_best_nn:.4f}\")\n",
        "print(f\"Precision: {precision_best_nn:.4f}\")\n",
        "print(f\"Recall: {recall_best_nn:.4f}\")\n",
        "print(f\"AUC: {best_auc_score:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_best_nn}\")"
      ],
      "metadata": {
        "id": "8AyLpOqNax-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f20de98"
      },
      "source": [
        "## Analysis of Neural Network Model Performance (Initial vs. Tuned)\n",
        "\n",
        "Let's compare the performance metrics of the initial Neural Network model (the one trained directly) with the best model found through manual hyperparameter tuning.\n",
        "\n",
        "### Initial Neural Network Model Performance:\n",
        "```\n",
        "Accuracy: 0.8891\n",
        "Precision: 0.9136\n",
        "Recall: 0.9603\n",
        "AUC: 0.8672\n",
        "Confusion Matrix:\n",
        "[[ 364  387]\n",
        " [ 169 4092]]\n",
        "```\n",
        "\n",
        "### Tuned Neural Network Model Performance:\n",
        "```\n",
        "Accuracy: 0.8915\n",
        "Precision: 0.9208\n",
        "Recall: 0.9545\n",
        "AUC: 0.8686\n",
        "Confusion Matrix:\n",
        "[[ 401  350]\n",
        " [ 194 4067]]\n",
        "```\n",
        "\n",
        "### Key Differences and Interpretation:\n",
        "\n",
        "1.  **Accuracy:**\n",
        "    *   **Initial:** 0.8891\n",
        "    *   **Tuned:** 0.8915\n",
        "    *   **Interpretation:** The tuned model shows a slight increase in overall accuracy. While the change is small, it indicates a marginally better proportion of correct predictions across both classes.\n",
        "\n",
        "2.  **Precision (for Class 1 - CompletedCourse):**\n",
        "    *   **Initial:** 0.9136\n",
        "    *   **Tuned:** 0.9208\n",
        "    *   **Interpretation:** Precision improved with tuning. This means that when the tuned model predicts a student will complete the course, it is correct more often (about 92.08% of the time, up from 91.36%). This indicates a reduction in false positives relative to true positives for the 'CompletedCourse' class.\n",
        "\n",
        "3.  **Recall (for Class 1 - CompletedCourse):**\n",
        "    *   **Initial:** 0.9603\n",
        "    *   **Tuned:** 0.9545\n",
        "    *   **Interpretation:** Recall for the 'CompletedCourse' class slightly decreased with tuning. The initial model was slightly better at identifying *all* students who actually completed the course. The tuned model missed a few more true completers (194 FN vs 169 FN initially).\n",
        "\n",
        "4.  **AUC (Area Under the ROC Curve):**\n",
        "    *   **Initial:** 0.8672\n",
        "    *   **Tuned:** 0.8686\n",
        "    *   **Interpretation:** AUC shows a modest improvement. This suggests the tuned model has slightly better discriminative power overall, meaning it's a bit better at distinguishing between students who complete and those who drop out.\n",
        "\n",
        "5.  **Confusion Matrix Analysis (Focus on Dropout - Class 0):**\n",
        "\n",
        "    *   **Initial Model:**\n",
        "        *   **TN (Correct Dropouts):** 364\n",
        "        *   **FP (Predicted Complete, Actual Dropout):** 387\n",
        "        *   **FN (Predicted Dropout, Actual Complete):** 169\n",
        "        *   **TP (Correct Completers):** 4092\n",
        "        *   **Recall for Dropout (Class 0):** 364 / (364 + 387) = 0.4847 (It identified about 48.5% of actual dropouts).\n",
        "        *   **Precision for Dropout (Class 0):** 364 / (364 + 169) = 0.6829 (When it predicted dropout, it was correct about 68.3% of the time).\n",
        "\n",
        "    *   **Tuned Model:**\n",
        "        *   **TN (Correct Dropouts):** 401\n",
        "        *   **FP (Predicted Complete, Actual Dropout):** 350\n",
        "        *   **FN (Predicted Dropout, Actual Complete):** 194\n",
        "        *   **TP (Correct Completers):** 4067\n",
        "        *   **Recall for Dropout (Class 0):** 401 / (401 + 350) = 0.5340 (It identified about 53.4% of actual dropouts).\n",
        "        *   **Precision for Dropout (Class 0):** 401 / (401 + 194) = 0.6739 (When it predicted dropout, it was correct about 67.4% of the time).\n",
        "\n",
        "### Overall Conclusion:\n",
        "\n",
        "The manual hyperparameter tuning of the Neural Network model led to a **better balance** between identifying dropouts and maintaining overall predictive quality.\n",
        "\n",
        "*   **Improved Dropout Detection:** The most notable improvement is in the **Recall for Dropout (Class 0)**, which increased from 0.4847 to 0.5340. This is a significant win for the business objective, as the model is now better at catching actual at-risk students (reducing False Negatives from the perspective of dropout being the positive class).\n",
        "*   **Reduced False Positives for CompletedCourse:** The number of students wrongly predicted to complete (False Positives for Class 1, which are missed dropouts) decreased from 387 to 350, further supporting the improved dropout detection.\n",
        "*   **Slight Trade-offs:** This improvement came with a slight decrease in Recall for the 'CompletedCourse' class (Class 1) and a minor decrease in Precision for the 'Dropout' class, meaning it might have slightly more false alarms for dropout. However, the gain in identifying actual dropouts (increased Recall for Class 0) often outweighs these minor trade-offs, depending on the cost of missing an actual dropout versus the cost of a false alarm.\n",
        "\n",
        "In summary, the tuning successfully made the Neural Network a more effective tool for identifying potential student dropouts, which is a critical goal for this project."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2 data"
      ],
      "metadata": {
        "id": "S77lqNrM_Gck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File URL\n",
        "file_url2 = \"https://drive.google.com/uc?id=1vy1JFQZva3lhMJQV69C43AB1NTM4W-DZ\""
      ],
      "metadata": {
        "id": "4Dg28A4C-Noo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 2: Pre-processing instructions**\n",
        "\n",
        "- Remove any columns not useful in the analysis (LearnerCode).\n",
        "- Remove columns with high cardinality (use >200 unique values, as a guideline for this data set).\n",
        "- Remove columns with >50% data missing.\n",
        "- Perform ordinal encoding for ordinal data.\n",
        "- Perform one-hot encoding for all other categorical data.\n",
        "- Choose how to engage with missing values, which can be done in one of two ways for this project:\n",
        "  *   Impute the rows with appropriate values.\n",
        "  *   Remove rows with missing values but ONLY in cases where rows with missing values are minimal: <2% of the overall data.\n",
        "\n"
      ],
      "metadata": {
        "id": "--AkGtUFEOq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start coding from here with Stage 2 dataset\n",
        "stage2 = pd.read_csv(file_url2)"
      ],
      "metadata": {
        "id": "DCL_MVZqmqZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage2.shape"
      ],
      "metadata": {
        "id": "JR-UQbwFy-NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage2.head()"
      ],
      "metadata": {
        "id": "2v3eb_DazC-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage2.describe()"
      ],
      "metadata": {
        "id": "XSnEEvCqzb3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage2 = stage2.drop(columns=['LearnerCode'])"
      ],
      "metadata": {
        "id": "et0_ZjoWzyK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in stage2.columns:\n",
        "    print(f\"Column '{col}': {stage2[col].nunique()} unique values\")"
      ],
      "metadata": {
        "id": "kQAwglHkz9Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove high cardinality columns\n",
        "stage2 = stage2.drop(columns=['HomeState', 'HomeCity', 'ProgressionDegree'])\n",
        "print(\"Shape of stage2 after removing high cardinality columns:\", stage2.shape)"
      ],
      "metadata": {
        "id": "VeIoAKfx0aTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#View missing data\n",
        "missing_data2 = pd.DataFrame({\n",
        "    'Missing Values': stage2.isnull().sum(),\n",
        "    'Percentage': (stage2.isnull().sum() / len(stage2)) * 100\n",
        "})\n",
        "print(missing_data2.sort_values(by='Missing Values', ascending=False))"
      ],
      "metadata": {
        "id": "3BQMZlYK0qdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage2=stage2.drop(columns='DiscountType')\n",
        "print(\"Shape of stage2 after removing features with majority missing values:\", stage2.shape)"
      ],
      "metadata": {
        "id": "5BXWGmmT1DJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a5766c5"
      },
      "source": [
        "# Calculate the median for 'AuthorisedAbsenceCount' and 'UnauthorisedAbsenceCount' from stage2\n",
        "median_authorised_absence = stage2['AuthorisedAbsenceCount'].median()\n",
        "median_unauthorised_absence = stage2['UnauthorisedAbsenceCount'].median()\n",
        "\n",
        "# Fill missing values with their respective medians\n",
        "stage2['AuthorisedAbsenceCount'] = stage2['AuthorisedAbsenceCount'].fillna(median_authorised_absence)\n",
        "stage2['UnauthorisedAbsenceCount'] = stage2['UnauthorisedAbsenceCount'].fillna(median_unauthorised_absence)\n",
        "\n",
        "print(\"Missing values in 'AuthorisedAbsenceCount' after imputation:\", stage2['AuthorisedAbsenceCount'].isnull().sum())\n",
        "print(\"Missing values in 'UnauthorisedAbsenceCount' after imputation:\", stage2['UnauthorisedAbsenceCount'].isnull().sum())\n",
        "\n",
        "# Verify the imputation by checking missing data again\n",
        "missing_data_after_imputation = pd.DataFrame({\n",
        "    'Missing Values': stage2.isnull().sum(),\n",
        "    'Percentage': (stage2.isnull().sum() / len(stage2)) * 100\n",
        "})\n",
        "print(\"\\nMissing data summary after imputation:\")\n",
        "print(missing_data_after_imputation.sort_values(by='Missing Values', ascending=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage2['CompletedCourse'] = stage2['CompletedCourse'].map({'Yes': 1, 'No': 0})\n",
        "print(stage2['CompletedCourse'].value_counts())"
      ],
      "metadata": {
        "id": "JpLLho6X2JMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ordinal_mapping = {\n",
        "    'Foundation': 0,\n",
        "    'International Year One': 1,\n",
        "    'International Year Two': 2,\n",
        "    'Pre-Masters': 3\n",
        "}\n",
        "stage2['CourseLevel'] = stage2['CourseLevel'].map(ordinal_mapping)\n",
        "print(\"Value counts after ordinal encoding for 'CourseLevel':\\n\", stage2['CourseLevel'].value_counts())"
      ],
      "metadata": {
        "id": "7x8xKylx2WSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nominal_cols = ['CentreName', 'BookingType', 'LeadSource', 'Gender', 'Nationality', 'CourseName', 'IsFirstIntake', 'ProgressionUniversity']\n",
        "\n",
        "# Perform one-hot encoding\n",
        "stage2_encoded = pd.get_dummies(stage2, columns=nominal_cols, drop_first=True)\n",
        "\n",
        "print(\"Shape of stage2 after one-hot encoding:\", stage2_encoded.shape)\n",
        "print(\"First 5 rows of the encoded DataFrame:\")\n",
        "print(stage2_encoded.head())"
      ],
      "metadata": {
        "id": "a2X6mnfh2gQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c84bded1"
      },
      "source": [
        "stage2_encoded['DateofBirth'] = pd.to_datetime(stage2_encoded['DateofBirth'], format='%d/%m/%Y', errors='coerce')\n",
        "current_year = pd.Timestamp.now().year\n",
        "stage2_encoded['Age'] = current_year - stage2_encoded['DateofBirth'].dt.year\n",
        "stage2_encoded = stage2_encoded.drop(columns=['DateofBirth'])\n",
        "\n",
        "print(\"Shape of stage2_encoded after processing DateofBirth:\", stage2_encoded.shape)\n",
        "print(\"First 5 rows of stage2_encoded after processing DateofBirth:\")\n",
        "print(stage2_encoded.head())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1694f0e1"
      },
      "source": [
        "#Split Stage 2 Data into Training and Test Sets\n",
        "X_stage2 = stage2_encoded.drop(columns=['CompletedCourse'])\n",
        "y_stage2 = stage2_encoded['CompletedCourse']\n",
        "\n",
        "X_train_stage2, X_test_stage2, y_train_stage2, y_test_stage2 = train_test_split(X_stage2, y_stage2, test_size=0.2, random_state=42, stratify=y_stage2)\n",
        "\n",
        "print(f\"X_train_stage2 shape: {X_train_stage2.shape}\")\n",
        "print(f\"X_test_stage2 shape: {X_test_stage2.shape}\")\n",
        "print(f\"y_train_stage2 shape: {y_train_stage2.shape}\")\n",
        "print(f\"y_test_stage2 shape: {y_test_stage2.shape}\")\n",
        "\n",
        "print(\"\\nDistribution of 'CompletedCourse' in original data:\\n\", y_stage2.value_counts(normalize=True))\n",
        "print(\"\\nDistribution of 'CompletedCourse' in training set:\\n\", y_train_stage2.value_counts(normalize=True))\n",
        "print(\"\\nDistribution of 'CompletedCourse' in test set:\\n\", y_test_stage2.value_counts(normalize=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60e0fc87"
      },
      "source": [
        "#Train XGBoost Model on Stage 2 Data\n",
        "xgb_model_stage2 = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
        "xgb_model_stage2.fit(X_train_stage2, y_train_stage2)\n",
        "\n",
        "print(\"XGBoost model for Stage 2 data instantiated and fitted successfully on the training data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fed28c14"
      },
      "source": [
        "# Make predictions on the test set for Stage 2 XGBoost model\n",
        "y_pred_stage2_xgb = xgb_model_stage2.predict(X_test_stage2)\n",
        "y_pred_proba_stage2_xgb = xgb_model_stage2.predict_proba(X_test_stage2)[:, 1] # Get probabilities for the positive class (1)\n",
        "\n",
        "# Calculate performance metrics for Stage 2 XGBoost model\n",
        "accuracy_stage2_xgb = accuracy_score(y_test_stage2, y_pred_stage2_xgb)\n",
        "precision_stage2_xgb = precision_score(y_test_stage2, y_pred_stage2_xgb)\n",
        "recall_stage2_xgb = recall_score(y_test_stage2, y_pred_stage2_xgb)\n",
        "roc_auc_stage2_xgb = roc_auc_score(y_test_stage2, y_pred_proba_stage2_xgb)\n",
        "conf_matrix_stage2_xgb = confusion_matrix(y_test_stage2, y_pred_stage2_xgb)\n",
        "\n",
        "# Print the performance indicators\n",
        "print(f\"XGBoost Model Performance on Stage 2 Test Set:\\n\")\n",
        "print(f\"Accuracy: {accuracy_stage2_xgb:.4f}\")\n",
        "print(f\"Precision: {precision_stage2_xgb:.4f}\")\n",
        "print(f\"Recall: {recall_stage2_xgb:.4f}\")\n",
        "print(f\"AUC: {roc_auc_stage2_xgb:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_stage2_xgb}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the features for Stage 2 (important for Neural Networks)\n",
        "scaler_stage2 = StandardScaler()\n",
        "X_train_stage2_scaled = scaler_stage2.fit_transform(X_train_stage2)\n",
        "X_test_stage2_scaled = scaler_stage2.transform(X_test_stage2)\n",
        "\n",
        "# Build the Neural Network model (using a similar architecture to the best Stage 1 NN for consistency)\n",
        "nn_model_stage2 = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_stage2_scaled.shape[1],)),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid') # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model (using SGD as it performed best in Stage 1 tuning for NN)\n",
        "nn_model_stage2.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
        "\n",
        "# Define Early Stopping callback\n",
        "early_stopping_stage2 = EarlyStopping(\n",
        "    monitor='val_loss', # Monitor validation loss\n",
        "    patience=10,        # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nTraining the Neural Network model for Stage 2 data...\")\n",
        "history_nn_stage2 = nn_model_stage2.fit(\n",
        "    X_train_stage2_scaled, y_train_stage2,\n",
        "    epochs=100, # Max epochs, EarlyStopping will stop it sooner if needed\n",
        "    batch_size=32,\n",
        "    validation_split=0.2, # Use a portion of the training data for validation\n",
        "    callbacks=[early_stopping_stage2],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nNeural Network model for Stage 2 data trained successfully.\")\n",
        "nn_model_stage2.summary()"
      ],
      "metadata": {
        "id": "-70jhiVMWYSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a99670ac"
      },
      "source": [
        "y_pred_nn_proba_stage2 = nn_model_stage2.predict(X_test_stage2_scaled)\n",
        "y_pred_nn_stage2 = (y_pred_nn_proba_stage2 > 0.5).astype(int) # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate performance metrics for Stage 2 Neural Network\n",
        "accuracy_nn_stage2 = accuracy_score(y_test_stage2, y_pred_nn_stage2)\n",
        "precision_nn_stage2 = precision_score(y_test_stage2, y_pred_nn_stage2)\n",
        "recall_nn_stage2 = recall_score(y_test_stage2, y_pred_nn_stage2)\n",
        "roc_auc_nn_stage2 = roc_auc_score(y_test_stage2, y_pred_nn_proba_stage2) # Use probabilities for AUC\n",
        "conf_matrix_nn_stage2 = confusion_matrix(y_test_stage2, y_pred_nn_stage2)\n",
        "\n",
        "# Print the performance indicators\n",
        "print(f\"Neural Network Model Performance on Stage 2 Test Set:\\n\")\n",
        "print(f\"Accuracy: {accuracy_nn_stage2:.4f}\")\n",
        "print(f\"Precision: {precision_nn_stage2:.4f}\")\n",
        "print(f\"Recall: {recall_nn_stage2:.4f}\")\n",
        "print(f\"AUC: {roc_auc_nn_stage2:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_nn_stage2}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "272b1d67"
      },
      "source": [
        "## Comparative Analysis of Model Performances (Stage 1 vs. Stage 2)\n",
        "\n",
        "### Stage 1 Model Performances:\n",
        "\n",
        "**XGBoost Model (Tuned - Stage 1):**\n",
        "*   **Accuracy:** 0.8960\n",
        "*   **Precision:** 0.9193\n",
        "*   **Recall:** 0.9622\n",
        "*   **AUC:** 0.8794\n",
        "*   **Confusion Matrix:** `[[ 391  360] [ 161 4100]]`\n",
        "*   **Recall for Dropout (Class 0):** 0.5206\n",
        "\n",
        "**Neural Network Model (Tuned - Stage 1):**\n",
        "*   **Accuracy:** 0.8915\n",
        "*   **Precision:** 0.9208\n",
        "*   **Recall:** 0.9545\n",
        "*   **AUC:** 0.8686\n",
        "*   **Confusion Matrix:** `[[ 401  350] [ 194 4067]]`\n",
        "*   **Recall for Dropout (Class 0):** 0.5340\n",
        "\n",
        "### Stage 2 Model Performances:\n",
        "\n",
        "**XGBoost Model (Stage 2):**\n",
        "*   **Accuracy:** 0.9058\n",
        "*   **Precision:** 0.9301\n",
        "*   **Recall:** 0.9615\n",
        "*   **AUC:** 0.9121\n",
        "*   **Confusion Matrix:** `[[ 443  308] [ 164 4097]]`\n",
        "*   **Recall for Dropout (Class 0):** 0.5901\n",
        "\n",
        "**Neural Network Model (Stage 2):**\n",
        "*   **Accuracy:** 0.8986\n",
        "*   **Precision:** 0.9295\n",
        "*   **Recall:** 0.9531\n",
        "*   **AUC:** 0.8844\n",
        "*   **Confusion Matrix:** `[[ 443  308] [ 200 4061]]`\n",
        "*   **Recall for Dropout (Class 0):** 0.5901\n",
        "\n",
        "### Key Comparisons and Insights:\n",
        "\n",
        "1.  **Impact of Additional Data (Stage 2 vs. Stage 1):**\n",
        "    *   **Overall Improvement:** Both XGBoost and Neural Network models show **improved performance** on the Stage 2 dataset compared to Stage 1. This indicates that the additional student engagement and absence data (AuthorisedAbsenceCount, UnauthorisedAbsenceCount) added in Stage 2 are valuable features for predicting student dropout.\n",
        "    *   **XGBoost:** Significant gains in AUC (0.8794 to 0.9121), Accuracy (0.8960 to 0.9058), and a notable increase in Recall for Dropout (0.5206 to 0.5901).\n",
        "    *   **Neural Network:** Also shows gains in AUC (0.8686 to 0.8844) and Accuracy (0.8915 to 0.8986). Recall for Dropout also improved (0.5340 to 0.5901).\n",
        "\n",
        "2.  **XGBoost vs. Neural Network:**\n",
        "    *   **XGBoost consistently outperforms the Neural Network** across both stages, particularly in terms of AUC. On Stage 2 data, XGBoost has a higher AUC (0.9121 vs. 0.8844), indicating better overall discriminatory power.\n",
        "    *   **Recall for Dropout (Class 0):** Both models achieved the same Recall for Dropout on Stage 2 (0.5901). This metric is critical for identifying at-risk students. While the XGBoost started lower in Stage 1, it caught up significantly with the NN in Stage 2.\n",
        "    *   **False Positives/Negatives for Dropout:** On Stage 2, both models identified the same number of True Negatives (443) and False Positives (308). However, the XGBoost model had slightly fewer False Negatives (164 vs. 200), meaning it missed fewer actual completers who were wrongly predicted to drop out.\n",
        "\n",
        "3.  **Specific Feature Importance (Implicit from Stage 2 Gains):**\n",
        "    *   The marked improvement in performance, especially in AUC and Recall for Dropout, strongly suggests that the absence count features (AuthorisedAbsenceCount, UnauthorisedAbsenceCount) are highly informative and powerful predictors for student dropout.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The inclusion of student engagement data in Stage 2 significantly enhances the predictive capabilities of both XGBoost and Neural Network models. The **XGBoost model emerges as the better performer** overall, demonstrating superior discriminative power (higher AUC) and a slightly better balance in identifying both completers and dropouts. The increased Recall for Dropout in Stage 2 for both models is a positive outcome, meaning they are better at identifying students who will likely drop out, allowing for more effective early intervention strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a919f0a5"
      },
      "source": [
        "## Explanation of Differences in Model Performance\n",
        "\n",
        "Impact of Additional Data (Stage 2 vs. Stage 1)\n",
        "\n",
        "*   **Enriched Feature Set:** The primary reason for the improved performance of both models from Stage 1 to Stage 2 is the **inclusion of more relevant and predictive features**. Stage 2 added student engagement data, specifically `AuthorisedAbsenceCount` and `UnauthorisedAbsenceCount`.\n",
        "*   **High Predictive Power of Absence Data:** Absences are often a direct indicator of disengagement or difficulties, which are strong precursors to student dropout. The models, particularly XGBoost, were able to leverage this new, highly informative data to make more accurate predictions. This is evident in the significant jump in AUC and Recall for Dropout for both models in Stage 2.\n",
        "*   **Better Signal for Minority Class:** The new features likely provided a much clearer signal for identifying the minority class (dropouts). Before, the models relied primarily on demographic and course information; now, they have behavioral data, which is often more directly linked to the target outcome.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b3a7390"
      },
      "source": [
        "param_dist_stage2 = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "    'max_depth': [3, 5, 7, 10]\n",
        "}\n",
        "\n",
        "print(\"Hyperparameter search space for Stage 2 defined successfully:\")\n",
        "print(param_dist_stage2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ee884c9"
      },
      "source": [
        "cv_stage2 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "random_search_stage2 = RandomizedSearchCV(\n",
        "    estimator=xgb.XGBClassifier(objective='binary:logistic', random_state=42),\n",
        "    param_distributions=param_dist_stage2,\n",
        "    n_iter=50,\n",
        "    scoring='roc_auc',\n",
        "    cv=cv_stage2,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Configuring and executing RandomizedSearchCV for Stage 2 XGBoost...\")\n",
        "random_search_stage2.fit(X_train_stage2, y_train_stage2)\n",
        "\n",
        "print(\"Best parameters found by RandomizedSearchCV for Stage 2:\")\n",
        "print(random_search_stage2.best_params_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e02e4da0"
      },
      "source": [
        "best_xgb_model_stage2 = xgb.XGBClassifier(**random_search_stage2.best_params_, objective='binary:logistic', random_state=42)\n",
        "best_xgb_model_stage2.fit(X_train_stage2, y_train_stage2)\n",
        "\n",
        "print(\"XGBoost model for Stage 2 instantiated with best parameters and retrained successfully.\")\n",
        "\n",
        "y_pred_tuned_stage2_xgb = best_xgb_model_stage2.predict(X_test_stage2)\n",
        "y_pred_proba_tuned_stage2_xgb = best_xgb_model_stage2.predict_proba(X_test_stage2)[:, 1]\n",
        "\n",
        "accuracy_tuned_stage2_xgb = accuracy_score(y_test_stage2, y_pred_tuned_stage2_xgb)\n",
        "precision_tuned_stage2_xgb = precision_score(y_test_stage2, y_pred_tuned_stage2_xgb)\n",
        "recall_tuned_stage2_xgb = recall_score(y_test_stage2, y_pred_tuned_stage2_xgb)\n",
        "roc_auc_tuned_stage2_xgb = roc_auc_score(y_test_stage2, y_pred_proba_tuned_stage2_xgb)\n",
        "conf_matrix_tuned_stage2_xgb = confusion_matrix(y_test_stage2, y_pred_tuned_stage2_xgb)\n",
        "\n",
        "print(f\"\\nXGBoost Tuned Model Performance on Stage 2 Test Set:\\n\")\n",
        "print(f\"Accuracy: {accuracy_tuned_stage2_xgb:.4f}\")\n",
        "print(f\"Precision: {precision_tuned_stage2_xgb:.4f}\")\n",
        "print(f\"Recall: {recall_tuned_stage2_xgb:.4f}\")\n",
        "print(f\"AUC: {roc_auc_tuned_stage2_xgb:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_tuned_stage2_xgb}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a81735ae"
      },
      "source": [
        "#Define Hyperparameter Combinations for Stage2 NN\n",
        "nn_param_combinations_stage2 = [\n",
        "    {\n",
        "        'n_neurons_l1': 128,\n",
        "        'n_neurons_l2': 64,\n",
        "        'n_neurons_l3': 32,\n",
        "        'activation': 'relu',\n",
        "        'dropout_rate': 0.3,\n",
        "        'optimizer': 'adam'\n",
        "    },\n",
        "    {\n",
        "        'n_neurons_l1': 256,\n",
        "        'n_neurons_l2': 128,\n",
        "        'n_neurons_l3': 64,\n",
        "        'activation': 'relu',\n",
        "        'dropout_rate': 0.4,\n",
        "        'optimizer': 'rmsprop'\n",
        "    },\n",
        "    {\n",
        "        'n_neurons_l1': 64,\n",
        "        'n_neurons_l2': 32,\n",
        "        'n_neurons_l3': 16,\n",
        "        'activation': 'sigmoid',\n",
        "        'dropout_rate': 0.2,\n",
        "        'optimizer': 'sgd'\n",
        "    },\n",
        "    {\n",
        "        'n_neurons_l1': 128,\n",
        "        'n_neurons_l2': 64,\n",
        "        'n_neurons_l3': 16,\n",
        "        'activation': 'relu',\n",
        "        'dropout_rate': 0.2,\n",
        "        'optimizer': 'adam'\n",
        "    },\n",
        "    {\n",
        "        'n_neurons_l1': 192,\n",
        "        'n_neurons_l2': 96,\n",
        "        'n_neurons_l3': 48,\n",
        "        'activation': 'sigmoid',\n",
        "        'dropout_rate': 0.3,\n",
        "        'optimizer': 'adam'\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Defined Neural Network hyperparameter combinations for Stage 2:\")\n",
        "for i, combo in enumerate(nn_param_combinations_stage2):\n",
        "    print(f\"Combination {i+1}: {combo}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99cac721"
      },
      "source": [
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "# Re-define build_nn_model function if not globally available, ensuring it's for Stage 2 features\n",
        "def build_nn_model(n_neurons_l1=128, n_neurons_l2=64, n_neurons_l3=32, activation='relu', dropout_rate=0.3, optimizer='adam'):\n",
        "    model = Sequential([\n",
        "        Dense(n_neurons_l1, activation=activation, input_shape=(X_train_stage2_scaled.shape[1],)),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(n_neurons_l2, activation=activation),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(n_neurons_l3, activation=activation),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
        "    return model\n",
        "\n",
        "# Re-define Early Stopping callback if not globally available\n",
        "early_stopping_stage2 = EarlyStopping(\n",
        "    monitor='val_loss', # Monitor validation loss\n",
        "    patience=10,        # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
        ")\n",
        "\n",
        "# Initialize an empty list to store trained models and their performance\n",
        "trained_nn_models_stage2 = []\n",
        "\n",
        "print(\"Starting Neural Network model training for all Stage 2 combinations...\")\n",
        "\n",
        "# Iterate through each dictionary in the nn_param_combinations_stage2 list\n",
        "for i, combo in enumerate(nn_param_combinations_stage2):\n",
        "    print(f\"\\nTraining model with combination {i+1}/{len(nn_param_combinations_stage2)}: {combo}\")\n",
        "\n",
        "    # Create an instance of KerasClassifier\n",
        "    nn_classifier_stage2 = KerasClassifier(\n",
        "        model=build_nn_model,\n",
        "        **combo,\n",
        "        epochs=100, # Max epochs, EarlyStopping will stop it sooner\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping_stage2],\n",
        "        verbose=0 # Suppress verbose output during training in the loop\n",
        "    )\n",
        "\n",
        "    # Fit the KerasClassifier model\n",
        "    nn_classifier_stage2.fit(X_train_stage2_scaled, y_train_stage2, validation_split=0.2)\n",
        "\n",
        "    # Store the trained model and its corresponding hyperparameters\n",
        "    trained_nn_models_stage2.append({\n",
        "        'params': combo,\n",
        "        'model': nn_classifier_stage2\n",
        "    })\n",
        "\n",
        "print(\"\\nNeural Network model training completed for all Stage 2 combinations.\")\n",
        "print(f\"Total trained models stored: {len(trained_nn_models_stage2)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "407e5591"
      },
      "source": [
        "best_nn_model_stage2 = None\n",
        "best_auc_score_stage2 = -1\n",
        "best_nn_params_stage2 = {}\n",
        "\n",
        "print(\"Evaluating trained Neural Network models for Stage 2...\")\n",
        "\n",
        "for i, model_info in enumerate(trained_nn_models_stage2):\n",
        "    model = model_info['model']\n",
        "    params = model_info['params']\n",
        "\n",
        "    # Predict probabilities on the scaled test set\n",
        "    y_pred_nn_proba_current = model.predict_proba(X_test_stage2_scaled)[:, 1]\n",
        "\n",
        "    # Calculate AUC score\n",
        "    current_auc = roc_auc_score(y_test_stage2, y_pred_nn_proba_current)\n",
        "\n",
        "    print(f\"\\nCombination {i+1} - Parameters: {params}\")\n",
        "    print(f\"AUC Score: {current_auc:.4f}\")\n",
        "\n",
        "    if current_auc > best_auc_score_stage2:\n",
        "        best_auc_score_stage2 = current_auc\n",
        "        best_nn_model_stage2 = model\n",
        "        best_nn_params_stage2 = params\n",
        "\n",
        "print(\"\\n--- Neural Network Model Evaluation for Stage 2 Complete ---\")\n",
        "print(f\"Best AUC Score: {best_auc_score_stage2:.4f}\")\n",
        "print(f\"Best Model Parameters: {best_nn_params_stage2}\")\n",
        "\n",
        "# Make final predictions with the best model\n",
        "y_pred_best_nn_proba_stage2 = best_nn_model_stage2.predict_proba(X_test_stage2_scaled)[:, 1]\n",
        "y_pred_best_nn_stage2 = (y_pred_best_nn_proba_stage2 > 0.5).astype(int)\n",
        "\n",
        "# Calculate full performance metrics for the best model\n",
        "accuracy_best_nn_stage2 = accuracy_score(y_test_stage2, y_pred_best_nn_stage2)\n",
        "precision_best_nn_stage2 = precision_score(y_test_stage2, y_pred_best_nn_stage2)\n",
        "recall_best_nn_stage2 = recall_score(y_test_stage2, y_pred_best_nn_stage2)\n",
        "conf_matrix_best_nn_stage2 = confusion_matrix(y_test_stage2, y_pred_best_nn_stage2)\n",
        "\n",
        "print(f\"\\nBest Neural Network Model Performance on Stage 2 Test Set:\\n\")\n",
        "print(f\"Accuracy: {accuracy_best_nn_stage2:.4f}\")\n",
        "print(f\"Precision: {precision_best_nn_stage2:.4f}\")\n",
        "print(f\"Recall: {recall_best_nn_stage2:.4f}\")\n",
        "print(f\"AUC: {best_auc_score_stage2:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_best_nn_stage2}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db0caa4b"
      },
      "source": [
        "## Analysis of Performance Improvement Post-Tuning on Stage 2\n",
        "\n",
        "### XGBoost Model Performance Comparison (Stage 2):\n",
        "\n",
        "**Untuned XGBoost (Stage 2):**\n",
        "*   **Accuracy:** 0.9058\n",
        "*   **Precision:** 0.9301\n",
        "*   **Recall:** 0.9615\n",
        "*   **AUC:** 0.9121\n",
        "*   **Confusion Matrix:** `[[ 443  308] [ 164 4097]]`\n",
        "*   **Recall for Dropout (Class 0):** 0.5901 (443 / (443 + 308))\n",
        "\n",
        "**Tuned XGBoost (Stage 2):**\n",
        "*   **Accuracy:** 0.9080\n",
        "*   **Precision:** 0.9299\n",
        "*   **Recall:** 0.9646\n",
        "*   **AUC:** 0.9129\n",
        "*   **Confusion Matrix:** `[[ 441  310] [ 151 4110]]`\n",
        "*   **Recall for Dropout (Class 0):** 0.5872 (441 / (441 + 310))\n",
        "\n",
        "**Comment on XGBoost Tuning (Stage 2):**\n",
        "Hyperparameter tuning for the XGBoost model on Stage 2 led to **marginal improvements**. While there was a slight increase in overall Accuracy (from 0.9058 to 0.9080) and Recall for Class 1 (CompletedCourse) (from 0.9615 to 0.9646), the Precision for Class 1 slightly decreased (0.9301 to 0.9299). The AUC score also saw a very minor increase (0.9121 to 0.9129). Interestingly, the Recall for Dropout (Class 0) slightly decreased (from 0.5901 to 0.5872). This suggests that the initial XGBoost model was already performing very well, and the selected tuning range provided only minor optimizations. The 'significant improvement' is not clearly evident here.\n",
        "\n",
        "### Neural Network Model Performance Comparison (Stage 2):\n",
        "\n",
        "**Untuned Neural Network (Stage 2):**\n",
        "*   **Accuracy:** 0.8986\n",
        "*   **Precision:** 0.9295\n",
        "*   **Recall:** 0.9531\n",
        "*   **AUC:** 0.8844\n",
        "*   **Confusion Matrix:** `[[ 443  308] [ 200 4061]]`\n",
        "*   **Recall for Dropout (Class 0):** 0.5901 (443 / (443 + 308))\n",
        "\n",
        "**Tuned Neural Network (Stage 2) (Best Model from manual search):**\n",
        "*   **Accuracy:** 0.8972\n",
        "*   **Precision:** 0.9239\n",
        "*   **Recall:** 0.9580\n",
        "*   **AUC:** 0.8901\n",
        "*   **Confusion Matrix:** `[[ 415  336] [ 179 4082]]`\n",
        "*   **Recall for Dropout (Class 0):** 0.5526 (415 / (415 + 336))\n",
        "\n",
        "**Comment on Neural Network Tuning (Stage 2):**\n",
        "For the Neural Network on Stage 2, hyperparameter tuning, in this manual iteration, resulted in a **mixed outcome with some trade-offs**. The AUC score did improve (from 0.8844 to 0.8901), indicating better overall discriminative power. Recall for Class 1 (CompletedCourse) also improved slightly (from 0.9531 to 0.9580). However, the Accuracy saw a slight decrease (0.8986 to 0.8972) and Precision for Class 1 also dropped (0.9295 to 0.9239). More notably, the **Recall for Dropout (Class 0) decreased** significantly (from 0.5901 to 0.5526). This means the tuned NN, while having better overall AUC, became less effective at identifying actual dropouts. This suggests that the chosen hyperparameter combinations, or the manual search approach, didn't necessarily find a configuration that universally improved all metrics, especially for the critical minority class recall. More extensive or systematic tuning (e.g., using `RandomizedSearchCV` for NN) might be needed to find a more balanced improvement.\n",
        "\n",
        "### Overall Conclusion on Tuning Impact (Stage 2):\n",
        "\n",
        "*   **XGBoost:** Tuning yielded **very small improvements**, suggesting the untuned model was already close to its optimal performance within the feature set. The gains are not 'significant' in a practical sense.\n",
        "*   **Neural Network:** The manual tuning led to an **increase in AUC** but resulted in a **decrease in the crucial Recall for Dropout (Class 0)**, indicating that the tuning process did not lead to a clear and significant performance improvement, and in some aspects, worsened performance for the minority class prediction. This highlights the complexity of NN tuning and the need for more systematic search strategies or a broader exploration of the hyperparameter space."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 3 data"
      ],
      "metadata": {
        "id": "wMe8QiJB_Kwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File URL\n",
        "file_url3 = \"https://drive.google.com/uc?id=18oyu-RQotQN6jaibsLBoPdqQJbj_cV2-\""
      ],
      "metadata": {
        "id": "i0Rigjfl-e5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 3: Pre-processing instructions**\n",
        "\n",
        "- Remove any columns not useful in the analysis (LearnerCode).\n",
        "- Remove columns with high cardinality (use >200 unique values, as a guideline for this data set).\n",
        "- Remove columns with >50% data missing.\n",
        "- Perform ordinal encoding for ordinal data.\n",
        "- Perform one-hot encoding for all other categorical data.\n",
        "- Choose how to engage with rows that have missing values, which can be done in one of two ways for this project:\n",
        "  *   Impute the rows with appropriate values.\n",
        "  *   Remove rows with missing values but ONLY in cases where rows with missing values are minimal: <2% of the overall data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nMN7RgGfHIcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start coding from here with Stage 3 dataset\n",
        "stage3 = pd.read_csv(file_url3)\n",
        "stage3.shape"
      ],
      "metadata": {
        "id": "dx-Ioa4OmbYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage3.head()"
      ],
      "metadata": {
        "id": "QfwTaZSdEMjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage3 = stage3.drop(columns=['LearnerCode'])"
      ],
      "metadata": {
        "id": "mybk36Z_EXdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in stage3.columns:\n",
        "    print(f\"Column '{col}': {stage3[col].nunique()} unique values\")"
      ],
      "metadata": {
        "id": "r0FcpLBOEmSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove high cardinality columns\n",
        "stage3 = stage3.drop(columns=['HomeState', 'HomeCity', 'ProgressionDegree'])\n",
        "print(\"Shape of stage3 after removing high cardinality columns:\", stage3.shape)"
      ],
      "metadata": {
        "id": "Pf18SlyNE6CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#View missing data\n",
        "missing_data = pd.DataFrame({\n",
        "    'Missing Values': stage3.isnull().sum(),\n",
        "    'Percentage': (stage3.isnull().sum() / len(stage3)) * 100\n",
        "})\n",
        "print(missing_data.sort_values(by='Missing Values', ascending=False))"
      ],
      "metadata": {
        "id": "_8ce5G70FMCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage3=stage3.drop(columns='DiscountType')\n",
        "print(\"Shape of stage3 after removing features with majority missing values:\", stage3.shape)"
      ],
      "metadata": {
        "id": "ZBb9juFiFfXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns to impute with median\n",
        "columns_to_impute = ['AssessedModules', 'PassedModules', 'FailedModules',\n",
        "                       'AuthorisedAbsenceCount', 'UnauthorisedAbsenceCount']\n",
        "\n",
        "for col in columns_to_impute:\n",
        "    median_value = stage3[col].median()\n",
        "    stage3[col] = stage3[col].fillna(median_value)\n",
        "\n",
        "print(\"Missing values after imputation:\")\n",
        "print(stage3[columns_to_impute].isnull().sum())\n",
        "\n",
        "# Verify the imputation by checking missing data again for the whole DataFrame\n",
        "missing_data_after_imputation = pd.DataFrame({\n",
        "    'Missing Values': stage3.isnull().sum(),\n",
        "    'Percentage': (stage3.isnull().sum() / len(stage3)) * 100\n",
        "})\n",
        "print(\"\\nFull missing data summary after imputation:\")\n",
        "print(missing_data_after_imputation.sort_values(by='Missing Values', ascending=False))"
      ],
      "metadata": {
        "id": "6uu9Tm0eFj4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage3['CompletedCourse'] = stage3['CompletedCourse'].map({'Yes': 1, 'No': 0})\n",
        "print(stage3['CompletedCourse'].value_counts())"
      ],
      "metadata": {
        "id": "Yz41q7ZBGPCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ordinal_mapping = {\n",
        "    'Foundation': 0,\n",
        "    'International Year One': 1,\n",
        "    'International Year Two': 2,\n",
        "    'Pre-Masters': 3\n",
        "}\n",
        "stage3['CourseLevel'] = stage3['CourseLevel'].map(ordinal_mapping)\n",
        "print(\"Value counts after ordinal encoding for 'CourseLevel':\\n\", stage3['CourseLevel'].value_counts())"
      ],
      "metadata": {
        "id": "FXUG1ONXGchx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nominal_cols = ['CentreName', 'BookingType', 'LeadSource', 'Gender', 'Nationality', 'CourseName', 'IsFirstIntake', 'ProgressionUniversity']\n",
        "\n",
        "# Perform one-hot encoding\n",
        "stage3_encoded = pd.get_dummies(stage3, columns=nominal_cols, drop_first=True)\n",
        "\n",
        "print(\"Shape of stage3 after one-hot encoding:\", stage3_encoded.shape)\n",
        "print(\"First 5 rows of the encoded DataFrame:\")\n",
        "print(stage3_encoded.head())"
      ],
      "metadata": {
        "id": "jw9yz1m3GiHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage3_encoded['DateofBirth'] = pd.to_datetime(stage3_encoded['DateofBirth'], format='%d/%m/%Y', errors='coerce')\n",
        "current_year = pd.Timestamp.now().year\n",
        "stage3_encoded['Age'] = current_year - stage3_encoded['DateofBirth'].dt.year\n",
        "stage3_encoded = stage3_encoded.drop(columns=['DateofBirth'])\n",
        "\n",
        "print(\"Shape of stage1_encoded after processing DateofBirth:\", stage3_encoded.shape)\n",
        "print(\"First 5 rows of stage3_encoded after processing DateofBirth:\")\n",
        "print(stage3_encoded.head())"
      ],
      "metadata": {
        "id": "HJ0dXQwVHHWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "796d632e"
      },
      "source": [
        "#Split Stage 3 Data into Training and Test Sets\n",
        "X_stage3 = stage3_encoded.drop(columns=['CompletedCourse'])\n",
        "y_stage3 = stage3_encoded['CompletedCourse']\n",
        "\n",
        "X_train_stage3, X_test_stage3, y_train_stage3, y_test_stage3 = train_test_split(X_stage3, y_stage3, test_size=0.2, random_state=42, stratify=y_stage3)\n",
        "\n",
        "print(f\"X_train_stage3 shape: {X_train_stage3.shape}\")\n",
        "print(f\"X_test_stage3 shape: {X_test_stage3.shape}\")\n",
        "print(f\"y_train_stage3 shape: {y_train_stage3.shape}\")\n",
        "print(f\"y_test_stage3 shape: {y_test_stage3.shape}\")\n",
        "\n",
        "print(\"\\nDistribution of 'CompletedCourse' in original data:\\n\", y_stage3.value_counts(normalize=True))\n",
        "print(\"\\nDistribution of 'CompletedCourse' in training set:\\n\", y_train_stage3.value_counts(normalize=True))\n",
        "print(\"\\nDistribution of 'CompletedCourse' in test set:\\n\", y_test_stage3.value_counts(normalize=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad38f332"
      },
      "source": [
        "#Train XGBoost Model on Stage 3 Data\n",
        "xgb_model_stage3 = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
        "xgb_model_stage3.fit(X_train_stage3, y_train_stage3)\n",
        "\n",
        "print(\"XGBoost model for Stage 3 data instantiated and fitted successfully on the training data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the features for Stage 3 (important for Neural Networks)\n",
        "scaler_stage3 = StandardScaler()\n",
        "X_train_stage3_scaled = scaler_stage3.fit_transform(X_train_stage3)\n",
        "X_test_stage3_scaled = scaler_stage3.transform(X_test_stage3)\n",
        "\n",
        "# Build the Neural Network model (using a similar architecture to Stage 1 initial model)\n",
        "nn_model_stage3 = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_stage3_scaled.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid') # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "nn_model_stage3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
        "\n",
        "# Define Early Stopping callback\n",
        "early_stopping_stage3 = EarlyStopping(\n",
        "    monitor='val_loss', # Monitor validation loss\n",
        "    patience=10,        # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nTraining the Neural Network model for Stage 3 data...\")\n",
        "history_nn_stage3 = nn_model_stage3.fit(\n",
        "    X_train_stage3_scaled, y_train_stage3,\n",
        "    epochs=100, # Max epochs, EarlyStopping will stop it sooner if needed\n",
        "    batch_size=32,\n",
        "    validation_split=0.2, # Use a portion of the training data for validation\n",
        "    callbacks=[early_stopping_stage3],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nNeural Network model for Stage 3 data trained successfully.\")\n",
        "nn_model_stage3.summary()"
      ],
      "metadata": {
        "id": "r3cDkhFMHa0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1899e48"
      },
      "source": [
        "y_pred_stage3_xgb = xgb_model_stage3.predict(X_test_stage3)\n",
        "y_pred_proba_stage3_xgb = xgb_model_stage3.predict_proba(X_test_stage3)[:, 1]\n",
        "\n",
        "accuracy_stage3_xgb = accuracy_score(y_test_stage3, y_pred_stage3_xgb)\n",
        "precision_stage3_xgb = precision_score(y_test_stage3, y_pred_stage3_xgb)\n",
        "recall_stage3_xgb = recall_score(y_test_stage3, y_pred_stage3_xgb)\n",
        "roc_auc_stage3_xgb = roc_auc_score(y_test_stage3, y_pred_proba_stage3_xgb)\n",
        "conf_matrix_stage3_xgb = confusion_matrix(y_test_stage3, y_pred_stage3_xgb)\n",
        "\n",
        "print(f\"XGBoost Model Performance on Stage 3 Test Set:\\n\")\n",
        "print(f\"Accuracy: {accuracy_stage3_xgb:.4f}\")\n",
        "print(f\"Precision: {precision_stage3_xgb:.4f}\")\n",
        "print(f\"Recall: {recall_stage3_xgb:.4f}\")\n",
        "print(f\"AUC: {roc_auc_stage3_xgb:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_stage3_xgb}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5205aa21"
      },
      "source": [
        "y_pred_nn_proba_stage3 = nn_model_stage3.predict(X_test_stage3_scaled)\n",
        "y_pred_nn_stage3 = (y_pred_nn_proba_stage3 > 0.5).astype(int) # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate performance metrics for Stage 3 Neural Network\n",
        "accuracy_nn_stage3 = accuracy_score(y_test_stage3, y_pred_nn_stage3)\n",
        "precision_nn_stage3 = precision_score(y_test_stage3, y_pred_nn_stage3)\n",
        "recall_nn_stage3 = recall_score(y_test_stage3, y_pred_nn_stage3)\n",
        "roc_auc_nn_stage3 = roc_auc_score(y_test_stage3, y_pred_nn_proba_stage3) # Use probabilities for AUC\n",
        "conf_matrix_nn_stage3 = confusion_matrix(y_test_stage3, y_pred_nn_stage3)\n",
        "\n",
        "# Print the performance indicators\n",
        "print(f\"Neural Network Model Performance on Stage 3 Test Set:\\n\")\n",
        "print(f\"Accuracy: {accuracy_nn_stage3:.4f}\")\n",
        "print(f\"Precision: {precision_nn_stage3:.4f}\")\n",
        "print(f\"Recall: {recall_nn_stage3:.4f}\")\n",
        "print(f\"AUC: {roc_auc_nn_stage3:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_nn_stage3}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17573468"
      },
      "source": [
        "## Comparative Analysis of Model Performances (Stage 2 vs. Stage 3)\n",
        "\n",
        "### Stage 2 Model Performances:\n",
        "\n",
        "**XGBoost Model (Untuned - Stage 2):**\n",
        "*   **Accuracy:** 0.9058\n",
        "*   **Precision:** 0.9301\n",
        "*   **Recall:** 0.9615\n",
        "*   **AUC:** 0.9121\n",
        "*   **Confusion Matrix:** `[[ 443  308] [ 164 4097]]`\n",
        "*   **Recall for Dropout (Class 0):** 0.5901 (calculated as 443 / (443 + 308))\n",
        "\n",
        "**Neural Network Model (Untuned - Stage 2):**\n",
        "*   **Accuracy:** 0.8986\n",
        "*   **Precision:** 0.9295\n",
        "*   **Recall:** 0.9531\n",
        "*   **AUC:** 0.8844\n",
        "*   **Confusion Matrix:** `[[ 443  308] [ 200 4061]]`\n",
        "*   **Recall for Dropout (Class 0):** 0.5901 (calculated as 443 / (443 + 308))\n",
        "\n",
        "### Stage 3 Model Performances:\n",
        "\n",
        "**XGBoost Model (Untuned - Stage 3):**\n",
        "*   **Accuracy:** 0.9729\n",
        "*   **Precision:** 0.9804\n",
        "*   **Recall:** 0.9878\n",
        "*   **AUC:** 0.9926\n",
        "*   **Confusion Matrix:** `[[ 667   84] [  52 4209]]`\n",
        "*   **Recall for Dropout (Class 0):** 0.8881 (calculated as 667 / (667 + 84))\n",
        "\n",
        "**Neural Network Model (Untuned - Stage 3):**\n",
        "*   **Accuracy:** 0.9599\n",
        "*   **Precision:** 0.9671\n",
        "*   **Recall:** 0.9864\n",
        "*   **AUC:** 0.9664\n",
        "*   **Confusion Matrix:** `[[ 608  143] [  58 4203]]`\n",
        "*   **Recall for Dropout (Class 0):** 0.8096 (calculated as 608 / (608 + 143))\n",
        "\n",
        "### Key Comparisons and Insights:\n",
        "\n",
        "**Impact of Additional Data (Stage 3 vs. Stage 2):**\n",
        "    *   **Significant Performance Boost:** The most striking observation is the **substantial improvement in performance for both models from Stage 2 to Stage 3**. This indicates that the academic performance data (`AssessedModules`, `PassedModules`, `FailedModules`) are incredibly powerful predictors of student dropout.\n",
        "    *   **XGBoost Gains:** Accuracy jumped from 0.9058 to 0.9729, AUC from 0.9121 to 0.9926, and crucially, Recall for Dropout (Class 0) from 0.5901 to 0.8881. This is a dramatic increase in identifying at-risk students.\n",
        "    *   **Neural Network Gains:** Accuracy rose from 0.8986 to 0.9599, AUC from 0.8844 to 0.9664, and Recall for Dropout from 0.5901 to 0.8096.\n",
        "    *   **Reduced Errors:** Both models saw a significant reduction in False Positives (missed dropouts) and False Negatives (false alarms for dropout) when comparing their confusion matrices from Stage 2 to Stage 3, indicating much more accurate classification across the board.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The inclusion of **academic performance data in Stage 3 proved to be the most critical factor** in significantly enhancing the predictive capabilities of both machine learning models. The models are now highly effective at predicting student dropout. The **XGBoost model consistently demonstrated superior performance**, particularly in its ability to identify actual dropouts (high Recall for Class 0) and its overall discriminative power (AUC). This makes the XGBoost model trained on the full Stage 3 dataset a powerful tool for Study Group to implement highly targeted and effective early intervention strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5eb6a7d"
      },
      "source": [
        "## Explanation of Differences in Model Performance\n",
        "\n",
        "Impact of Additional Data (Stage 3 vs. Stage 2)\n",
        "\n",
        "*   **Enriched Feature Set:** The primary reason for the significant improvement in performance for both models from Stage 2 to Stage 3 is the **inclusion of highly predictive academic performance data**. Stage 3 introduced `AssessedModules`, `PassedModules`, and `FailedModules`.\n",
        "*   **Direct Indicators of Outcome:** Academic performance metrics are often very direct and powerful indicators of student success or failure. They provide a much clearer signal about a student's likelihood to drop out than purely demographic or even engagement data.\n",
        "*   **Stronger Signal for Minority Class:** These new features provided a much stronger and more distinct signal for identifying the minority class (dropouts). It's easier for models to learn to differentiate between students who pass all their modules and those who fail several, making dropout prediction significantly more accurate.\n",
        "\n",
        "\n",
        "**Specific Observations from the Stage 3 models:**\n",
        "\n",
        "*   **XGBoost's Superior Dropout Detection (Recall for Class 0):** With an 88.81% recall for dropout, XGBoost was significantly better at identifying actual dropouts compared to the Neural Network's 80.96%. This difference is crucial for intervention strategies.\n",
        "*   **Overall Discriminative Power (AUC):** XGBoost's AUC of 0.9926 (nearly perfect) indicates its probabilities were much better calibrated and it had a clearer separation between classes compared to the NN's 0.9664. This is a very strong indicator of a superior model for this task.\n",
        "\n",
        "In conclusion, the academic performance data proved to be a game-changer, dramatically improving the predictive power of both models. However, the XGBoost model consistently demonstrated its robust capabilities and efficiency in exploiting these tabular features, ultimately achieving a slightly higher, more balanced, and more impactful performance, particularly in identifying at-risk students."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "745e8688"
      },
      "source": [
        "param_dist_stage3 = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "    'max_depth': [3, 5, 7, 10]\n",
        "}\n",
        "\n",
        "print(\"Hyperparameter search space for Stage 3 defined successfully:\")\n",
        "print(param_dist_stage3)\n",
        "\n",
        "cv_stage3 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "random_search_stage3 = RandomizedSearchCV(\n",
        "    estimator=xgb.XGBClassifier(objective='binary:logistic', random_state=42),\n",
        "    param_distributions=param_dist_stage3,\n",
        "    n_iter=50,\n",
        "    scoring='roc_auc',\n",
        "    cv=cv_stage3,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Configuring and executing RandomizedSearchCV for Stage 3 XGBoost...\")\n",
        "random_search_stage3.fit(X_train_stage3, y_train_stage3)\n",
        "\n",
        "print(\"Best parameters found by RandomizedSearchCV for Stage 3:\")\n",
        "print(random_search_stage3.best_params_)\n",
        "\n",
        "best_xgb_model_stage3 = xgb.XGBClassifier(**random_search_stage3.best_params_, objective='binary:logistic', random_state=42)\n",
        "best_xgb_model_stage3.fit(X_train_stage3, y_train_stage3)\n",
        "\n",
        "print(\"XGBoost model for Stage 3 instantiated with best parameters and retrained successfully.\")\n",
        "\n",
        "y_pred_tuned_stage3_xgb = best_xgb_model_stage3.predict(X_test_stage3)\n",
        "y_pred_proba_tuned_stage3_xgb = best_xgb_model_stage3.predict_proba(X_test_stage3)[:, 1]\n",
        "\n",
        "accuracy_tuned_stage3_xgb = accuracy_score(y_test_stage3, y_pred_tuned_stage3_xgb)\n",
        "precision_tuned_stage3_xgb = precision_score(y_test_stage3, y_pred_tuned_stage3_xgb)\n",
        "recall_tuned_stage3_xgb = recall_score(y_test_stage3, y_pred_tuned_stage3_xgb)\n",
        "roc_auc_tuned_stage3_xgb = roc_auc_score(y_test_stage3, y_pred_proba_tuned_stage3_xgb)\n",
        "conf_matrix_tuned_stage3_xgb = confusion_matrix(y_test_stage3, y_pred_tuned_stage3_xgb)\n",
        "\n",
        "print(f\"\\nXGBoost Tuned Model Performance on Stage 3 Test Set:\\n\")\n",
        "print(f\"Accuracy: {accuracy_tuned_stage3_xgb:.4f}\")\n",
        "print(f\"Precision: {precision_tuned_stage3_xgb:.4f}\")\n",
        "print(f\"Recall: {recall_tuned_stage3_xgb:.4f}\")\n",
        "print(f\"AUC: {roc_auc_tuned_stage3_xgb:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_tuned_stage3_xgb}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5781757b"
      },
      "source": [
        "nn_param_combinations_stage3 = [\n",
        "    {\n",
        "        'n_neurons_l1': 128,\n",
        "        'n_neurons_l2': 64,\n",
        "        'n_neurons_l3': 32,\n",
        "        'activation': 'relu',\n",
        "        'dropout_rate': 0.3,\n",
        "        'optimizer': 'adam'\n",
        "    },\n",
        "    {\n",
        "        'n_neurons_l1': 256,\n",
        "        'n_neurons_l2': 128,\n",
        "        'n_neurons_l3': 64,\n",
        "        'activation': 'relu',\n",
        "        'dropout_rate': 0.4,\n",
        "        'optimizer': 'rmsprop'\n",
        "    },\n",
        "    {\n",
        "        'n_neurons_l1': 64,\n",
        "        'n_neurons_l2': 32,\n",
        "        'n_neurons_l3': 16,\n",
        "        'activation': 'sigmoid',\n",
        "        'dropout_rate': 0.2,\n",
        "        'optimizer': 'sgd'\n",
        "    },\n",
        "    {\n",
        "        'n_neurons_l1': 128,\n",
        "        'n_neurons_l2': 64,\n",
        "        'n_neurons_l3': 16,\n",
        "        'activation': 'relu',\n",
        "        'dropout_rate': 0.2,\n",
        "        'optimizer': 'adam'\n",
        "    },\n",
        "    {\n",
        "        'n_neurons_l1': 192,\n",
        "        'n_neurons_l2': 96,\n",
        "        'n_neurons_l3': 48,\n",
        "        'activation': 'sigmoid',\n",
        "        'dropout_rate': 0.3,\n",
        "        'optimizer': 'adam'\n",
        "    } # New combination added\n",
        "]\n",
        "\n",
        "print(\"Defined Neural Network hyperparameter combinations for Stage 3:\")\n",
        "for i, combo in enumerate(nn_param_combinations_stage3):\n",
        "    print(f\"Combination {i+1}: {combo}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71c02bff"
      },
      "source": [
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "# Re-define build_nn_model function if not globally available, ensuring it's for Stage 3 features\n",
        "def build_nn_model(n_neurons_l1=128, n_neurons_l2=64, n_neurons_l3=32, activation='relu', dropout_rate=0.3, optimizer='adam'):\n",
        "    model = Sequential([\n",
        "        Dense(n_neurons_l1, activation=activation, input_shape=(X_train_stage3_scaled.shape[1],)),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(n_neurons_l2, activation=activation),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(n_neurons_l3, activation=activation),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
        "    return model\n",
        "\n",
        "# Re-define Early Stopping callback if not globally available\n",
        "early_stopping_stage3 = EarlyStopping(\n",
        "    monitor='val_loss', # Monitor validation loss\n",
        "    patience=10,        # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
        ")\n",
        "\n",
        "# Initialize an empty list to store trained models and their performance\n",
        "trained_nn_models_stage3 = []\n",
        "\n",
        "print(\"Setup complete: build_nn_model function defined, early stopping callback defined, and trained_nn_models_stage3 list initialized.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9956856e"
      },
      "source": [
        "print(\"Starting Neural Network model training for all Stage 3 combinations...\")\n",
        "\n",
        "# Iterate through each dictionary in the nn_param_combinations_stage3 list\n",
        "for i, combo in enumerate(nn_param_combinations_stage3):\n",
        "    print(f\"\\nTraining model with combination {i+1}/{len(nn_param_combinations_stage3)}: {combo}\")\n",
        "\n",
        "    # Create an instance of KerasClassifier\n",
        "    # Pass epochs and callbacks to the KerasClassifier init\n",
        "    nn_classifier_stage3 = KerasClassifier(\n",
        "        model=build_nn_model,\n",
        "        **combo,\n",
        "        epochs=100, # Max epochs, EarlyStopping will stop it sooner\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping_stage3],\n",
        "        verbose=0 # Suppress verbose output during training in the loop\n",
        "    )\n",
        "\n",
        "    # Fit the KerasClassifier model\n",
        "    nn_classifier_stage3.fit(X_train_stage3_scaled, y_train_stage3, validation_split=0.2)\n",
        "\n",
        "    # Store the trained model and its corresponding hyperparameters\n",
        "    trained_nn_models_stage3.append({\n",
        "        'params': combo,\n",
        "        'model': nn_classifier_stage3\n",
        "    })\n",
        "\n",
        "print(\"\\nNeural Network model training completed for all Stage 3 combinations.\")\n",
        "print(f\"Total trained models stored: {len(trained_nn_models_stage3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a5aca55"
      },
      "source": [
        "best_nn_model_stage3 = None\n",
        "best_auc_score_stage3 = -1\n",
        "best_nn_params_stage3 = {}\n",
        "\n",
        "print(\"Evaluating trained Neural Network models for Stage 3...\")\n",
        "\n",
        "for i, model_info in enumerate(trained_nn_models_stage3):\n",
        "    model = model_info['model']\n",
        "    params = model_info['params']\n",
        "\n",
        "    # Predict probabilities on the scaled test set\n",
        "    y_pred_nn_proba_current = model.predict_proba(X_test_stage3_scaled)[:, 1]\n",
        "\n",
        "    # Calculate AUC score\n",
        "    current_auc = roc_auc_score(y_test_stage3, y_pred_nn_proba_current)\n",
        "\n",
        "    print(f\"\\nCombination {i+1} - Parameters: {params}\")\n",
        "    print(f\"AUC Score: {current_auc:.4f}\")\n",
        "\n",
        "    if current_auc > best_auc_score_stage3:\n",
        "        best_auc_score_stage3 = current_auc\n",
        "        best_nn_model_stage3 = model\n",
        "        best_nn_params_stage3 = params\n",
        "\n",
        "print(\"\\n--- Neural Network Model Evaluation for Stage 3 Complete ---\")\n",
        "print(f\"Best AUC Score: {best_auc_score_stage3:.4f}\")\n",
        "print(f\"Best Model Parameters: {best_nn_params_stage3}\")\n",
        "\n",
        "# Make final predictions with the best model\n",
        "y_pred_best_nn_proba_stage3 = best_nn_model_stage3.predict_proba(X_test_stage3_scaled)[:, 1]\n",
        "y_pred_best_nn_stage3 = (y_pred_best_nn_proba_stage3 > 0.5).astype(int)\n",
        "\n",
        "# Calculate full performance metrics for the best model\n",
        "accuracy_best_nn_stage3 = accuracy_score(y_test_stage3, y_pred_best_nn_stage3)\n",
        "precision_best_nn_stage3 = precision_score(y_test_stage3, y_pred_best_nn_stage3)\n",
        "recall_best_nn_stage3 = recall_score(y_test_stage3, y_pred_best_nn_stage3)\n",
        "conf_matrix_best_nn_stage3 = confusion_matrix(y_test_stage3, y_pred_best_nn_stage3)\n",
        "\n",
        "print(f\"\\nBest Neural Network Model Performance on Stage 3 Test Set:\\n\")\n",
        "print(f\"Accuracy: {accuracy_best_nn_stage3:.4f}\")\n",
        "print(f\"Precision: {precision_best_nn_stage3:.4f}\")\n",
        "print(f\"Recall: {recall_best_nn_stage3:.4f}\")\n",
        "print(f\"AUC: {best_auc_score_stage3:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_best_nn_stage3}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c74f5c3"
      },
      "source": [
        "## Analysis of Performance Improvement Post-Tuning on Stage 3\n",
        "\n",
        "### XGBoost Model Performance Comparison (Stage 3):\n",
        "\n",
        "**Untuned XGBoost (Stage 3):**\n",
        "*   **Accuracy:** 0.9729\n",
        "*   **Precision:** 0.9804\n",
        "*   **Recall:** 0.9878\n",
        "*   **AUC:** 0.9926\n",
        "*   **Confusion Matrix:** `[[ 667   84] [  52 4209]]`\n",
        "\n",
        "**Tuned XGBoost (Stage 3):**\n",
        "*   **Accuracy:** 0.9769\n",
        "*   **Precision:** 0.9821\n",
        "*   **Recall:** 0.9908\n",
        "*   **AUC:** 0.9935\n",
        "*   **Confusion Matrix:** `[[ 674   77] [  39 4222]]`\n",
        "\n",
        "**Comment on XGBoost Tuning (Stage 3):**\n",
        "Hyperparameter tuning for the XGBoost model on Stage 3 resulted in **minor, but consistent improvements** across most metrics. Accuracy increased from 0.9729 to 0.9769, Precision from 0.9804 to 0.9821, Recall from 0.9878 to 0.9908, and AUC from 0.9926 to 0.9935. The confusion matrix also shows a slight reduction in false positives (from 84 to 77) and false negatives (from 52 to 39). While the improvements are not dramatic, this is likely because the untuned XGBoost model on Stage 3 data was already performing exceptionally well, with an AUC close to perfect. The tuning helped to slightly refine this already high performance, demonstrating the model's robustness and the effectiveness of the added academic data.\n",
        "\n",
        "### Neural Network Model Performance Comparison (Stage 3):\n",
        "\n",
        "**Untuned Neural Network (Stage 3):**\n",
        "*   **Accuracy:** 0.9599\n",
        "*   **Precision:** 0.9671\n",
        "*   **Recall:** 0.9864\n",
        "*   **AUC:** 0.9664\n",
        "*   **Confusion Matrix:** `[[ 608  143] [  58 4203]]`\n",
        "\n",
        "**Tuned Neural Network (Stage 3):**\n",
        "*   **Accuracy:** 0.9641\n",
        "*   **Precision:** 0.9683\n",
        "*   **Recall:** 0.9901\n",
        "*   **AUC:** 0.9745\n",
        "*   **Confusion Matrix:** `[[ 613  138] [  42 4219]]`\n",
        "\n",
        "**Comment on Neural Network Tuning (Stage 3):**\n",
        "Hyperparameter tuning for the Neural Network on Stage 3 yielded **noticeable improvements**. Accuracy increased from 0.9599 to 0.9641, Precision from 0.9671 to 0.9683, and Recall from 0.9864 to 0.9901. Most significantly, the AUC improved from 0.9664 to 0.9745, indicating a better overall ability to distinguish between classes. The confusion matrix also shows a reduction in False Positives (from 143 to 138) and False Negatives (from 58 to 42), which is beneficial for minimizing misclassifications. This indicates that the manual tuning process successfully identified better hyperparameters for the Neural Network on the rich Stage 3 dataset.\n",
        "\n",
        "### Overall Conclusion on Tuning Impact (Stage 3):\n",
        "\n",
        "*   **XGBoost:** Hyperparameter tuning provided **slight, incremental improvements** to an already highly effective model. The untuned model was already near optimal due to the strength of the Stage 3 features.\n",
        "*   **Neural Network:** Hyperparameter tuning led to **more substantial improvements** across key metrics, particularly AUC and a reduction in both false positives and false negatives. This indicates that the chosen hyperparameters were more suitable for leveraging the Stage 3 data effectively, bringing the Neural Network's performance closer to that of XGBoost."
      ]
    }
  ]
}